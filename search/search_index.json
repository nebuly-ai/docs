{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quickstart","text":"<p><code>Nebullvm</code>\u00a0is an ecosystem of plug and play modules to boost the performances of your AI systems. The optimization modules are stack-agnostic and work with any library. They are designed to be easily integrated into your system, providing a quick and seamless boost to its performance. Simply plug and play to start realizing the benefits of optimized performance right away.</p> <p>If you like the idea, give us a star to show your support for the project\u00a0\u2b50</p>"},{"location":"#what-can-this-help-with","title":"What can this help with?","text":"<p>There are multiple modules we actually provide to boost the performances of your AI systems:</p> <ul> <li> <p> Speedster: Automatically apply the best set of SOTA optimization techniques to achieve the maximum inference speed-up on your hardware.</p> </li> <li> <p> Nos: Automatically maximize the utilization of GPU resources in a Kubernetes cluster through real-time dynamic partitioning and elastic quotas - Effortless optimization at its finest!</p> </li> <li> <p> ChatLLaMA: Build faster and cheaper ChatGPT-like training process based on LLaMA architectures.</p> </li> <li> <p> OpenAlphaTensor: Increase the computational performances of an AI model with custom-generated matrix multiplication algorithm fine-tuned for your specific hardware.</p> </li> <li> <p> Forward-Forward: The Forward Forward algorithm is a method for training deep neural networks that replaces the backpropagation forward and backward passes with two forward passes.</p> </li> </ul>"},{"location":"#next-modules-and-roadmap","title":"Next modules and roadmap","text":"<p>We are actively working on incorporating the following modules, as requested by members of our community, in upcoming releases:</p> <ul> <li> Promptify: Effortlessly personalize large APIs generative models from OpenAI, Cohere, HF to your specific context and requirements.</li> <li> CloudSurfer: Automatically discover the optimal cloud configuration and hardware on AWS, GCP and Azure to run your AI models.</li> <li> OptiMate: Interactive tool guiding savvy users in achieving the best inference performance out of a given model / hardware setup.</li> <li> TrainingSim: Easily simulate the training of large AI models on a distributed infrastructure to predict training behaviours without actual implementation.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>As an open source project in a rapidly evolving field, we welcome contributions of all kinds, including new features, improved infrastructure, and better documentation. If you're interested in contributing, please see the linked page for more information on how to get involved.</p> <p> Join the community |   Visit the website </p>"},{"location":"contributions/","title":"Contribution guidelines","text":"<p>We are glad that you decided to contribute to the library and we thank you for your efforts. Below we briefly lay out the main guidelines for conforming your code to the coding style we have adopted for the project.</p>"},{"location":"contributions/#how-to-contribute","title":"How to contribute","text":"<p>We adopt the GitHub workflow, so you can procede as follows:</p> <ol> <li>Fork the repo</li> <li>Clone your fork</li> <li>Create a branch for the feature you want to implement<ul> <li><code>git checkout -b my-feature</code></li> <li>keep the branch small!</li> </ul> </li> <li>Once you\u2019re finished, open a pull request to the original repository</li> </ol>"},{"location":"contributions/#how-to-submit-an-issue","title":"How to submit an issue","text":"<p>Did you spot a bug? Did you come up with a cool idea that you think should be implemented in the libraries? Well, GitHub issues are the best way to let us know! We don't have a strict policy on issue generation: just use a meaningful title and specify the problem or your proposal in the first problem comment. Then, you can use GitHub labels to let us know what kind of proposal you are making, for example bug if you are reporting a new bug or enhancement if you are proposing a library improvement. Please remeber to specifiy the module to which the issue is related (e.g Speedster, Promptify, etc). If it's one of your first contributions, check the tag <code>good first issue</code> \ud83c\udfc1 and start coding.</p> <p>Happy coding \ud83d\udcab</p>"},{"location":"Speedster/advanced_options/","title":"Advanced options","text":"<p>If you\u2019re new to the library, you may want to start with the Getting started section.</p> <p>The user guide here shows more advanced workflows and how to use the library in different ways. We are going to show some examples of more advanced usages of <code>Speedster</code>, that we hope will give you a deeper insight of how <code>Speedster</code> works. </p> <p>In particular, we will overview:</p> <ul> <li><code>optimize_model</code> API</li> <li>Acceleration suggestions</li> <li>Selecting which device to use for the optimization: CPU and GPU</li> <li>Optimization Time: constrained vs unconstrained</li> <li>Selecting specific compilers/compressors</li> <li>Using dynamic shape</li> <li>Enable TensorrtExecutionProvider for ONNXRuntime on GPU</li> <li>Use TensorRT Plugins to boost Stable Diffusion optimization on GPU</li> <li>Custom models</li> <li>Store the performances of all the optimization techniques</li> <li>Set number of threads</li> </ul>"},{"location":"Speedster/advanced_options/#optimize_model-api","title":"<code>optimize_model</code> API","text":"<p>The <code>optimize_model</code> function allows to optimize a model from one of the supported frameworks (PyTorch, HuggingFace, TensorFlow, ONNX), and returns an optimized model that can be used with the same interface as the original model.</p> <pre><code>def optimize_model(\nmodel: Any,\ninput_data: Union[Iterable, Sequence],\nmetric_drop_ths: Optional[float] = None,\nmetric: Union[str, (...) -&gt; Any, None] = None,\noptimization_time: str = \"constrained\",\ndynamic_info: Optional[dict] = None,\nconfig_file: Optional[str] = None,\nignore_compilers: Optional[List[str]] = None,\nignore_compressors: Optional[List[str]] = None,\nstore_latencies: bool = False,\ndevice: str = None,\n**kwargs: Any\n) -&gt; Any\n</code></pre> <p>Arguments</p> <p><code>model</code>: Any</p> <p>The input model can belong to one of the following frameworks: PyTorch, TensorFlow, ONNX, HuggingFace. In the ONNX case, <code>model</code> is a string with the path to the saved onnx model. In the other cases, it is a torch.nn.Module or a tf.Module.</p> <p><code>input_data</code>: Iterable or Sequence</p> <p>Input data needed to test the optimization performances (latency, throughput, accuracy loss, etc). It can consist of one or more data samples. Note that if <code>optimization_time</code> is set to \"unconstrained,\" it would be preferable to provide at least 100 data samples to also activate <code>Speedster</code> techniques that require more data (pruning, etc.). See the Getting started section to learn more about the <code>input_data</code> depending on your input framework:</p> <ul> <li>Getting started with PyTorch optimization</li> <li>Getting started with \ud83e\udd17 HuggingFace optimization</li> <li>Getting started with Stable Diffusion optimization</li> <li>Getting started with TensorFlow/Keras optimization</li> <li>Getting started with ONNX optimization</li> </ul> <p><code>metric_drop_ths</code>: float, optional</p> <p>Maximum drop in your preferred metric (see \"metric\" section below). All the optimized models having a larger error with respect to the <code>metric_drop_ths</code> will be discarded. </p> <p>Default: 0.</p> <p><code>metric</code>: Callable, optional</p> <p>Metric to be used for estimating the error that may arise from using optimization techniques and for evaluating if the error exceeds the <code>metric_drop_ths</code>.  <code>metric</code> accepts as input a string, a user-defined metric, or None. Metric accepts a string containing the name of the metric; it currently supports:</p> <ul> <li>\"numeric_precision\"</li> <li>\"accuracy\". </li> <li>user-defined metric: function that takes as input the output of the original model and the one of the optimized model, and, if available, the original label. The function calculates and returns the reduction in the metric due to the optimization. </li> </ul> <p>Default: \"numeric_precision\". </p> <p><code>optimization_time</code>: OptimizationTime, optional</p> <p>The optimization time mode. It can be \"constrained\" or \"unconstrained\". In \"constrained\" mode, Speedster takes advantage only of compilers and precision reduction techniques, such as quantization. \"unconstrained\" optimization_time allows it to exploit more time-consuming techniques, such as pruning and distillation. Note that most techniques activated in \"unconstrained\" mode require fine-tuning, and therefore it is recommended to provide at least 100 samples as input_data. </p> <p>Default: \"constrained\".</p> <p><code>dynamic_info</code>: Dict, optional</p> <p>Dictionary containing dynamic axis information. It should contain as keys both \"input\" and \"output\" and as values two lists of dictionaries, where each dictionary represents dynamic axis information for an input/output tensor. The inner dictionary should have an integer as a key, i.e. the dynamic axis (also considering the batch size) and a string as a value giving it a tag, e.g., \"batch_size.\". </p> <p>Default: None.</p> <p><code>config_file</code>: str, optional</p> <p>Configuration file containing the parameters needed to define the CompressionStep in the pipeline. </p> <p>Default: None.</p> <p><code>ignore_compilers</code>: List[str], optional</p> <p>List of DL compilers ignored during optimization execution. The compiler name should be one among tvm, tensor RT, openvino, onnxruntime, deepsparse, tflite, bladedisc, torchscript, intel_neural_compressor . </p> <p>Default: None.</p> <p><code>ignore_compressors</code>: List[str], optional</p> <p>List of DL compressors ignored during the compression stage. The compressor name should be one among sparseml and intel_pruning. </p> <p>Default: None.</p> <p><code>store_latencies</code>: bool, optional</p> <p>Parameter that allows to store the latency for each compiler used by Speedster in a json file. The JSON is created in the working directory. </p> <p>Default: False.</p> <p><code>device</code>: str, optional</p> <p>Device used for inference, it can be cpu or gpu/cuda (both gpu and cuda options are supported). A specific gpu can be selected using notation gpu:1 or cuda:1. gpu will be used if available, otherwise cpu. </p> <p>Default: None.</p> <p>Returns: Inference Learner</p> <p>Optimized version with the same interface of the input model. For example, optimizing a PyTorch model will return an InferenceLearner object that can be called exactly like a PyTorch model (either with model.forward(input) or model(input)). The optimized model will therefore take as input a torch.Tensors and return a torch.Tensors.</p>"},{"location":"Speedster/advanced_options/#acceleration-suggestions","title":"Acceleration suggestions","text":"<p>If the speedup you obtained with the first optimization with <code>Speedster</code> is not enough, we suggest the following actions:</p> <ul> <li>Include more backends for optimization, i.e. set <code>--backend all</code></li> <li>Increase the <code>metric_drop_ths</code> by 5%, if possible: see Optimize_model API</li> <li>Verify that your device is supported by your version of speedster: see Supported hardware</li> <li>Try to accelerate your model on a different hardware or consider using the CloudSurfer module to automatically understand which is the best hardware for your model: see CloudSurfer module.</li> </ul>"},{"location":"Speedster/advanced_options/#selecting-which-device-to-use-cpu-and-gpu","title":"Selecting which device to use: CPU and GPU","text":"<p>The parameter <code>device</code> allows to select which device we want to use for inference. By default, <code>Speedster</code> will use the gpu if available on the machine, otherwise it will use cpu. If we are running on a machine with a gpu available and we want to optimize the model for cpu inference, we can use:</p> <pre><code>from speedster import optimize_model\noptimized_model = optimize_model(\nmodel, input_data=input_data, device=\"cpu\"\n)\n</code></pre> <p>If we are working on a multi-gpu machine and we want to use a specific gpu, we can use:</p> <pre><code>from speedster import optimize_model\noptimized_model = optimize_model(\nmodel, input_data=input_data, device=\"cuda:1\"  # also device=\"gpu:1\" is supported\n)\n</code></pre>"},{"location":"Speedster/advanced_options/#optimization-time-constrained-vs-unconstrained","title":"Optimization Time: constrained vs unconstrained","text":"<p>One of the first options that can be customized in <code>Speedster</code> is the <code>optimization_time</code> parameter. In order to optimize the model, <code>Speedster</code> will try a list of compilers which allow to keep the same accuracy of the original model. In addition to compilers, it can also use other techniques such as pruning, quantization, and other compression techniques which can lead to a little drop in accuracy and may require some time to complete. </p> <p>We defined two scenarios:</p> <ul> <li> <p>constrained: only compilers and precision reduction techniques are used, so the compression step (the most time consuming one) is skipped. Moreover, in some cases the same compiler could be available for more than one pipeline, for example tensor RT is available both with PyTorch and ONNX backends. In the constrained scenario, each compiler will be used only once, so if for example we optimize a PyTorch model and tensor RT in the PyTorch pipeline manages to optimize the model, it won't be used again in the ONNX pipeline.</p> </li> <li> <p>unconstrained: in this scenario, <code>Speedster</code> will use all the compilers available, even if they appear in more than one backend. It also allows the usage of more time consuming techniques such as pruning and distillation. Note that for using many of the sophisticated techniques in the 'unconstrained' optimization, a small fine-tuning of the model will be needed. Thus, we highly recommend to provide as input_data at least 100 samples when selecting 'unconstrained' optimization.</p> </li> </ul>"},{"location":"Speedster/advanced_options/#select-specific-compilerscompressors","title":"Select specific compilers/compressors","text":"<p>The <code>optimize_model</code> functions accepts also the parameters <code>ignore_compilers</code> and <code>ignore_compressors</code>, which allow to skip specific compilers or compressors. For example, if we want to skip the <code>tvm</code> and <code>bladedisc</code> optimizers, we could write:</p> <pre><code>from speedster import optimize_model\noptimized_model = optimize_model(\nmodel, \ninput_data=input_data, \nignore_compilers=[\"tvm\", \"bladedisc\"]\n)\n# You can find the list of all compilers and compressors below\n# COMPILER_LIST = [\n#     \"deepsparse\",\n#     \"tensor_rt\",  # Skips all the tensor RT pipelines\n#     \"torch_tensor_rt\",  # Skips only the tensor RT pipeline for PyTorch\n#     \"onnx_tensor_rt\",  # Skips only the tensor RT pipeline for ONNX\n#     \"torchscript\",\n#     \"onnxruntime\",\n#     \"tflite\",\n#     \"tvm\",  # Skips all the TVM pipelines\n#     \"onnx_tvm\",  # Skips only the TVM pipeline for ONNX\n#     \"torch_tvm\",  # Skips only the TVM pipeline for PyTorch\n#     \"openvino\",\n#     \"bladedisc\",\n#     \"intel_neural_compressor\",\n# ]\n# \n# COMPRESSOR_LIST = [\n#     \"sparseml\",\n#     \"intel_pruning\",\n# ]\n</code></pre>"},{"location":"Speedster/advanced_options/#using-dynamic-shape","title":"Using dynamic shape","text":"<p>By default, a model optimized with <code>Speedster</code> will have a static shape. This means that it can be used in inference only with the same shape of the inputs provided to the <code>optimize_model</code> function during the optimization. The dynamic shape however is fully supported, and can be enabled with the <code>dynamic_info</code> parameter (see the optimize_model API arguments to see how this parameter is defined.)</p> <p>For each dynamic axis in the inputs, we need to provide the following information: - the axis number (starting from 0, considering the batch size as the first axis) - a tag that will be used to identify the axis - the minimum, optimal and maximum sizes of the axis (some compilers will work also for shapes that are not in the range [min, max], but the performance may be worse)</p> <p>Let's see an example of a model that takes two inputs, where the batch size must be dynamic, as well as the size on the third and fourth dimensions.</p> <pre><code>import torch\nimport torchvision.models as models\nfrom speedster import optimize_model\n# Load a resnet as example\nmodel = models.resnet50()\n# Provide an input data for the model\ninput_data = [((torch.randn(1, 3, 256, 256),), torch.tensor([0])) for _ in range(100)]\n# Set dynamic info\ndynamic_info = {\n\"inputs\": [\n{\n0: {\n\"name\": \"batch\",\n\"min_val\": 1,\n\"opt_val\": 1,\n\"max_val\": 8,\n}, \n2: {\n\"name\": \"dim_image\",\n\"min_val\": 128,\n\"opt_val\": 256,\n\"max_val\": 512,\n}, \n3: {\n\"name\": \"dim_image\",\n\"min_val\": 128,\n\"opt_val\": 256,\n\"max_val\": 512,\n}, \n}\n],\n\"outputs\": [\n{0: \"batch\", 1: \"out_dim\"}\n]\n}\n# Run Speedster optimization in one line of code\noptimized_model = optimize_model(\nmodel, \ninput_data=input_data, \noptimization_time=\"constrained\", \ndynamic_info=dynamic_info\n)\n</code></pre>"},{"location":"Speedster/advanced_options/#enable-tensorrtexecutionprovider-for-onnxruntime-on-gpu","title":"Enable TensorrtExecutionProvider for ONNXRuntime on GPU","text":"<p>By default, <code>Speedster</code> will use the <code>CUDAExecutionProvider</code> for ONNXRuntime on GPU. If you want to use the <code>TensorrtExecutionProvider</code> instead, you must add the TensorRT installation path to the env variable LD_LIBRARY_PATH. If you installed TensorRT through the nebullvm auto_installer, you can do it by running the following command in the terminal:</p> <pre><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:\"/&lt;PATH_TO_PYTHON_FOLDER&gt;/site-packages/tensorrt\"\n</code></pre>"},{"location":"Speedster/advanced_options/#use-tensorrt-plugins-to-boost-stable-diffusion-optimization-on-gpu","title":"Use TensorRT Plugins to boost Stable Diffusion optimization on GPU","text":"<p>To achieve the best results on GPU for Stable diffusion models, we have to activate the TensorRT Plugins. Speedster supports their usage on Stable Diffusion models from version 0.9.0, you can see this guide to set them up:  Setup TensorRT Plugins</p>"},{"location":"Speedster/advanced_options/#custom-models","title":"Custom models","text":"<p><code>Speedster</code> is designed to optimize models that take as inputs and return in output only tensors or np.ndarrays (and dictionaries/strings for huggingface). Some models may require instead a custom input, for example a dictionary where the keys are the names of the inputs and the values are the input tensors, or may return a dictionary as output. We can optimize such models with <code>Speedster</code> by defining a model wrapper.</p> <p>Let's take the example of the detectron2 model which takes as input a tuple of tensors but returns a dictionary as output:</p> <pre><code> class BaseModelWrapper(torch.nn.Module):\ndef __init__(self, core_model, output_dict):\nsuper().__init__()\nself.core_model = core_model\nself.output_names = [key for key in output_dict.keys()]\ndef forward(self, *args, **kwargs):\nres = self.core_model(*args, **kwargs)\nreturn tuple(res[key] for key in self.output_names)\nclass OptimizedWrapper(torch.nn.Module):\ndef __init__(self, optimized_model, output_keys):\nsuper().__init__()\nself.optimized_model = optimized_model\nself.output_keys = output_keys\ndef forward(self, *args):\nres = self.optimized_model(*args)\nreturn {key: value for key, value in zip(self.output_keys, res)}\ninput_data = [((torch.randn(1, 3, 256, 256)), torch.tensor([0]))]\n# Compute the original output of the model (in dict format) \nres = model_backbone(torch.randn(1, 3, 256, 256))\n# Pass the model and the output sample to the wrapper\nbackbone_wrapper = BaseModelWrapper(model_backbone, res)\n# Optimize the model wrapper\noptimized_model = optimize_model(backbone_wrapper, input_data=input_data)\n# Wrap the optimized model with a new wrapper to restore the original model output format\noptimized_backbone = OptimizedWrapper(optimized_model, backbone_wrapper.output_names)\n</code></pre> <p>You can find other examples in the notebooks section available on GitHub.</p>"},{"location":"Speedster/advanced_options/#store-the-performances-of-all-the-optimization-techniques","title":"Store the performances of all the optimization techniques","text":"<p><code>Speedster</code> internally tries all the techniques available on the target hardware and automatically chooses the fastest one. If you need more details on the inference times of each compiler, you can set the <code>store_latencies</code> parameter to <code>True</code>. A json file will be created in the working directory, listing all the results of the applied techniques and of the original model itself.</p> <pre><code># Run Speedster optimization in one line of code\noptimized_model = optimize_model(\nmodel, \ninput_data=input_data, \nstore_latencies=True\n)\n</code></pre>"},{"location":"Speedster/advanced_options/#set-number-of-threads","title":"Set number of threads","text":"<p>When running multiple replicas of the model in parallel, it would be useful for CPU-optimized algorithms to limit the number of threads to use for each model. In <code>Speedster</code>, it is possible to set the maximum number of threads a single model can use with the environment variable <code>NEBULLVM_THREADS_PER_MODEL</code>. </p> <p>For instance, you can run:</p> <pre><code>export NEBULLVM_THREADS_PER_MODEL = 2\n</code></pre> <p>for using just two CPU threads per model at inference time and during optimization.</p>"},{"location":"Speedster/benchmarks/","title":"Benchmarks","text":"<p>Info</p> <p>In this section you are going to learn how <code>Speedster</code> accelerates the inference of various models on different hardware architecture.</p> <p>Here we provide a preview of the following accelerated models:</p> <ul> <li>Bert</li> <li>YoloV5</li> <li>EfficientNet</li> <li>GPT2</li> <li>ResNet</li> <li>Roberta</li> </ul> <p>The above models are tested on very popular hardware architecture and instances:</p> <ul> <li>AWS - c5n,2xlarge</li> <li>AWS - c5,12xlarge</li> <li>AWS - c6i.12xlarge</li> <li>AWS - m6i,24xlarge</li> <li>NVIDIA T4</li> <li>NVIDIA V100</li> <li>NVIDIA 3090</li> </ul>"},{"location":"Speedster/benchmarks/#bert","title":"Bert","text":""},{"location":"Speedster/benchmarks/#yolov5","title":"YoloV5","text":""},{"location":"Speedster/benchmarks/#efficientnet","title":"EfficientNet","text":""},{"location":"Speedster/benchmarks/#gpt2","title":"GPT2","text":""},{"location":"Speedster/benchmarks/#resnet","title":"ResNet","text":""},{"location":"Speedster/benchmarks/#roberta","title":"Roberta","text":""},{"location":"Speedster/hardware/","title":"Supported hardware","text":"<p><code>Speedster</code> has been mostly tested on Nvidia GPUs and Intel/AMD CPUs. The library may also work with other hardware on which has not been tested. Please let us know if you find out that <code>Speedster</code> works well on other hardware or if you find issues.</p> <p>Fully supported hardware:</p> <ul> <li>Intel CPU</li> <li>Nvidia GPU</li> </ul> <p>Hardware we are currently integrating:</p> <ul> <li>Apple M1</li> <li>AMD CPU</li> <li>Intel GPU (open issue \ud83d\udc69\u200d\ud83d\udcbb)</li> </ul>"},{"location":"Speedster/installation/","title":"Installation","text":"<p>In this installation guide we will learn:</p> <ul> <li> <p>Quick installation of <code>Speedster</code> with pip (Recommended) </p> </li> <li> <p>Selective installation of the requirements (Optional) </p> </li> <li> <p>Installation with docker (Optional) </p> </li> </ul>"},{"location":"Speedster/installation/#quick-installation","title":"Quick installation","text":"<p>You can easily install <code>Speedster</code> using pip.</p> <pre><code>pip install speedster\n</code></pre> <p>Then make sure to install all the available deep learning compilers:</p> <pre><code>python -m nebullvm.installers.auto_installer --compilers all\n</code></pre> <p>Info</p> <p>If you want to optimize PyTorch or HuggingFace models, PyTorch must be pre-installed in the environment before using the auto-installer, please install it from this link. Moreover, for Mac computers with M1/M2 processors, please use a conda environment, or you may run into problems when installing some of the deep learning compilers.</p> <p>Great, now you are now ready to accelerate your model \ud83d\ude80 Please visit the following pages to get started based on the DL framework of your input model:</p> <ul> <li>Getting started with PyTorch optimization</li> <li>Getting started with \ud83e\udd17 HuggingFace optimization</li> <li>Getting started with Stable Diffusion optimization</li> <li>Getting started with TensorFlow/Keras optimization</li> <li>Getting started with ONNX optimization</li> </ul>"},{"location":"Speedster/installation/#optional-selective-installation-of-speedster-requirements","title":"(Optional) Selective installation of Speedster requirements","text":"<p>By default, the <code>auto_installer</code> installs all the DL frameworks and compilers supported by <code>Speedster</code>. However, some of these may not be relevant to your use case. In this section, we explain how you can customize the installation of these libraries, avoiding those that are not needed.</p> <p>To customize the libraries installation you have two options:</p> <ul> <li>Use the auto-installer (recommended)</li> <li>Install the libraries manually</li> </ul>"},{"location":"Speedster/installation/#use-the-auto-installer-recommended","title":"Use the auto-installer (recommended)","text":"<p>To understand how to selectively install your preferred libraries, let's examine the auto-installer API:</p> <pre><code>python -m nebullvm.installers.auto_installer \n--frameworks &lt;frameworks&gt; \n--extra-backends &lt;backends&gt; \n--compilers &lt;compilers&gt;\n</code></pre> <p>Description</p> --frameworks--extra-backends--compilers <p><code>frameworks</code> is used to specify the deep learning framework of your input model. The supported frameworks are <code>torch</code>, <code>tensorflow</code>, <code>onnx</code>, <code>huggingface</code> and <code>diffusers</code>.</p> <ul> <li> <p>if you want to optimize a model with a single DL framework, the code is as follows (example below for HuggingFace):</p> <pre><code>python -m nebullvm.installers.auto_installer --frameworks huggingface\n</code></pre> <p>Please remember that for PyTorch optimization, you should pre-install PyTorch from the official repo.</p> </li> <li> <p>if you want to optimize models in multiple input frameworks, you can include them separated with a space:     <pre><code>python -m nebullvm.installers.auto_installer --frameworks tensorflow torch\n</code></pre></p> </li> <li> <p>If you want to include all the frameworks, you can use <code>all</code> as the argument:</p> <pre><code>python -m nebullvm.installers.auto_installer --frameworks all\n</code></pre> </li> </ul> <p>Default: <code>all</code>.</p> <p>After entering your input model, <code>Speedster</code> converts the input model from its original framework into an intermediate framework to be used during the optimization; we call these intermediate frameworks \"backends.\" To learn more, see the section Model Converter in the docs. This conversion allows <code>Speedster</code> to apply all optimization techniques without being constrained by the input framework of your model.</p> <p>The supported backends are <code>torch</code>, <code>tensorflow</code> and <code>onnx</code>.</p> <p>You can specify multiple backends by separating them with a space. </p> <ul> <li> <p>For example, if you want to install TensorFlow and ONNX as backends of an HugginFace model, the code is as follows:</p> <p><code>python python -m nebullvm.installers.auto_installer --frameworks huggingface --extra-backends tensorflow onnx</code>python</p> </li> <li> <p>If you want to install all the backends supported by the selected frameworks, you can use <code>all</code> as the argument.</p> </li> <li>If you don't want to install extra backends, you can set <code>--extra-backends none</code>.</li> </ul> <p>The extra-backends that you choose must be compatible with at least one of the input frameworks you previously selected with the argument <code>\u2014-frameworks</code>, please see the table below to see the compatibility matrix. </p> <p>Default: <code>all</code>.    </p> <p><code>compilers</code> is used to specify the deep learning compilers to be installed. The supported compilers are: <code>deepsparse</code>, <code>tensor_rt</code>, <code>torch_tensor_rt</code>, <code>openvino</code> and <code>intel_neural_compressor</code>. The compilers must be compatible with at least one of the backends selected with the argument <code>\u2014-extra-backends</code>, please see the table below to see the compatibility matrix.</p> <ul> <li> <p>You can specify multiple compilers by separating them with a space. For example:</p> <pre><code>--compilers deepsparse tensor_rt\n</code></pre> <p>will install DeepSparse and TensorRT. </p> </li> <li> <p>If you want to install all the compilers supported by the selected frameworks/backends, you can use <code>all</code> as the argument.</p> </li> </ul> <p>Speedster also supports <code>torchscript</code>, <code>tf_lite</code>, and <code>onnxruntime</code> as built-in; these are preinstalled with their respective backends, so there is no need to include them in the list. Speedster also supports <code>tvm</code>, which is currently not supported by the automatic installer and must be installed manually; see the next section if you wish to include it.</p> <p>Default: <code>all</code>.</p> <p>Let's see an example of how to use these three arguments:</p> <pre><code>python -m nebullvm.installers.auto_installer \n--frameworks torch \n--extra-backends all \n--compilers all\n</code></pre> <p>This command will setup your environment to optimize PyTorch models, and will install all PyTorch supported backends and compilers.</p> <p>The following table shows the supported combinations of frameworks, backends and compilers that you can install with the auto-installer:</p> Framework Extra Backends Compilers PyTorch ONNX DeepSparse, TensorRT, Torch TensorRT, OpenVINO, Intel Neural Compressor TensorFlow ONNX TensorRT, OpenVINO ONNX / TensorRT, OpenVINO HuggingFace PyTorch, TensorFlow, ONNX DeepSparse, TensorRT, Torch TensorRT, OpenVINO, Intel Neural Compressor Diffusers PyTorch, ONNX DeepSparse, TensorRT, Torch TensorRT, OpenVINO, Intel Neural Compressor <p>Info</p> <p>Hugginface models can be of two types, PyTorch-based or TensorFlow-based. For PyTorch-based models, it is necessary to include <code>torch</code> as an extra-backend. For TensorFlow-based models, you must include <code>tensorflow</code> as an extra-backend.</p>"},{"location":"Speedster/installation/#manual-installation","title":"Manual installation","text":"<p>If you want to manually install the requirements, this section collects links to the official installation guides for all frameworks and compilers supported by <code>Speedster</code>.</p>"},{"location":"Speedster/installation/#deep-learning-frameworksbackends","title":"Deep Learning frameworks/backends","text":"<ul> <li>PyTorch: https://pytorch.org/get-started/locally/</li> <li>TensorFlow: https://www.tensorflow.org/install</li> <li>ONNX: https://github.com/onnx/onnx#installation</li> <li>HuggingFace: https://huggingface.co/transformers/installation.html</li> <li>Diffusers: https://github.com/huggingface/diffusers#installation</li> </ul>"},{"location":"Speedster/installation/#deep-learning-compilers","title":"Deep Learning compilers","text":"<ul> <li>DeepSparse: https://github.com/neuralmagic/deepsparse#installation</li> <li>TensorRT: https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html</li> <li>Torch TensorRT: https://pytorch.org/TensorRT/getting_started/installation.html#installation</li> <li>ONNXRuntime: https://onnxruntime.ai/docs/install/#python-installs</li> <li>OpenVINO: https://docs.openvino.ai/latest/openvino_docs_install_guides_install_dev_tools.html#step-4-install-the-package</li> <li>Intel Neural Compressor: https://github.com/intel/neural-compressor#installation</li> <li>Apache TVM: https://tvm.apache.org/docs/install/index.html</li> </ul>"},{"location":"Speedster/installation/#other-requirements","title":"Other requirements","text":"<ul> <li>tf2onnx: https://github.com/onnx/tensorflow-onnx#installation (Install it if you want to convert TensorFlow models to ONNX)</li> <li>polygraphy: https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy#installation (Install it if you want to use TensorRT)</li> <li>onnx-simplifier: https://github.com/daquexian/onnx-simplifier#python-version (Install it if you want to use TensorRT)</li> <li>onnx_graphsurgeon: https://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeon#installation (Install it if you want to use TensorRT plugins with Stable Diffusion)</li> <li>onnxmltools: https://github.com/onnx/onnxmltools#install (Install it if you want to convert models to ONNX)</li> </ul>"},{"location":"Speedster/installation/#optional-download-docker-images-with-frameworks-and-optimizers","title":"(Optional) Download Docker images with frameworks and optimizers","text":"<p>Instead of installing the frameworks and compilers needed for optimization, which can be a time-consuming task, you can simply download a docker container with all compilers preinstalled.</p> <p>To pull up the docker image, run:</p> <pre><code>docker pull nebulydocker/nebullvm:latest\n</code></pre> <p>and then run and access the docker with:</p> <pre><code>docker run -ti --gpus=all nebulydocker/nebullvm:latest\n</code></pre> <p>After optimizing the model, you may decide to deploy it to production. Note that you need to have the deep learning compiler used to optimize the model and other components inside the production docker. For this reason, we have created several versions of the Docker nebullvm container in the Docker Hub, each containing only one compiler. Pull the image with the compiler that has optimized your model!</p>"},{"location":"Speedster/key_concepts/","title":"Key concepts","text":"<p>In this section we are going to learn the architectural design of the 4 building blocks of <code>Speedster</code>.</p> <ul> <li> Converter: converts the input model from its original framework to the framework backends supported by Speedster, namely PyTorch, TensorFlow, and ONNX. This allows the Compressor and Optimizer modules to apply any optimization technique to the model.</li> <li> Compressor: applies various compression techniques to the model, such as pruning, knowledge distillation, or quantization-aware training.</li> <li> Optimizer: converts the compressed models to the intermediate representation (IR) of the supported deep learning compilers. The compilers apply both post-training quantization techniques and graph optimizations, to produce compiled binary files.</li> <li> Inference Learner: takes the best performing compiled model and converts it to the same interface as the original input model.</li> </ul> <p></p> <p>The\u00a0compressor\u00a0stage leverages the following open-source projects:</p> <ul> <li>Intel/neural-compressor: targeting to provide unified APIs for network compression technologies, such as low precision quantization, sparsity, pruning, knowledge distillation, across different deep learning frameworks to pursue optimal inference performance.</li> <li>SparseML: libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models.</li> </ul> <p>The\u00a0compiler stage\u00a0leverages the following open-source projects:</p> <ul> <li>Apache TVM: open deep learning compiler stack for cpu, gpu and specialized accelerators.</li> <li>BladeDISC: end-to-end Dynamic Shape Compiler project for machine learning workloads.</li> <li>DeepSparse: neural network inference engine that delivers GPU-class performance for sparsified models on CPUs.</li> <li>OpenVINO: open-source toolkit for optimizing and deploying AI inference.</li> <li>ONNX Runtime: cross-platform, high performance ML inferencing and training accelerator</li> <li>TensorRT: C++ library for high performance inference on NVIDIA GPUs and deep learning accelerators.</li> <li>TFlite\u00a0and\u00a0XLA: open-source libraries to accelerate TensorFlow models.</li> </ul>"},{"location":"Speedster/key_concepts/#model-converter","title":"Model converter","text":"<p>Definition</p> <p>The Converter converts the input model from its original input framework to the framework backends supported by <code>Speedster</code>. This conversion enables the Compressor and the Compiler modules to apply all the optimization techniques without being constrained by the framework of your input model.</p> <p></p> <p><code>Speedster</code> supports deep learning models in the following input frameworks:</p> <ul> <li>Hugging Face</li> <li>Diffusers</li> <li>ONNX</li> <li>PyTorch</li> <li>TensorFlow</li> </ul> <p><code>Speedster</code> now includes 3 backends:</p> <ul> <li>ONNX backend, which supports models in any input framework.</li> <li>PyTorch backend, which supports input models in PyTorch and ONNX and Hugging Face. </li> <li>TensorFlow backend, which supports input models in TensorFlow and ONNX.</li> </ul> <p>As you notice, to date, not all cross-conversions from input frameworks to each <code>Speedster</code> backend are supported. </p> <p>Let's see a couple of examples to better understand the potenatiality of the Converter block:</p> <ol> <li> <p>PyTorch model as input: first of all Speedster will try the compilers available in the PyTorch backend pipeline, then it will convert it to ONNX and will try also the ones available in the ONNX backend optimization pipeline. Finally, the best one among them will be chosen and returned as the optimized model in your input framework (in this case PyTorch).</p> </li> <li> <p>HuggingFace model as input: Let's assume that for your specific use case, the best optimization technique is a specific type of dynamic quantization only supported by PyTorch. If you feed a Hugging Face model into Speedster, the Converter will first transform your model into a PyTorch model. Speedster will then quantize it and finally return it as an Hugging Face model.</p> </li> </ol>"},{"location":"Speedster/key_concepts/#compressor","title":"Compressor","text":"<p>The compressor applies various compression techniques to the model:</p> <ul> <li>Block-wise un/structured sparsity (\ud83c\udf89 launched in 0.4.0 \ud83c\udf89)</li> <li>Knowledge distillation (to be supported)</li> <li>Layer replacement (to be supported)</li> <li>Low-rank compression (to be supported)</li> <li>Quantization-aware training (to be supported)</li> <li>SparseML (\ud83c\udf89 launched in 0.4.0 \ud83c\udf89)</li> </ul> <p></p>"},{"location":"Speedster/key_concepts/#compiler","title":"Compiler","text":"<p>The Compiler block converts the compressed models to the intermediate representation (IR) of the supported deep learning compilers. The different DL compilers perform both the low-level optimizations, which mostly consist of various quantization techniques, and graph optimizations. Finally, the model is compiled into binary.</p> <p></p> <p>Supported deep learning compilers:</p> <ul> <li>Apache TVM</li> <li>BladeDISC (\ud83c\udf89 launched in 0.4.0 \ud83c\udf89)</li> <li>DeepSparse (\ud83c\udf89 launched in 0.4.0 \ud83c\udf89)</li> <li>MLIR (open pull request \ud83d\udc69\u200d\ud83d\udcbb)</li> <li>ONNX Runtime</li> <li>OpenVINO</li> <li>TensorRT</li> <li>TF Lite / XLA</li> <li>TorchScript</li> </ul> <p>Supported low-level optimizations:</p> <ul> <li>Static quantization</li> <li>Dynamic quantization</li> <li>Half-precision</li> <li>Low-bit quantization on TVM (to be supported)</li> </ul>"},{"location":"Speedster/key_concepts/#inference-learner","title":"Inference learner","text":"<p>The Learner, or Inference Learner, selects the most performing compiled model on your hardware and converts it to the same interface as the original input model.</p> <p></p>"},{"location":"Speedster/notebooks/","title":"Notebooks","text":"<p>In this section you can find optimization notebooks for multiple DL input models:</p> <ul> <li>HuggingFace</li> <li>Diffusers</li> <li>ONNX</li> <li>Pytorch</li> <li>Tensorflow</li> </ul> <p>Please check out notebooks and tutorials on GitHub at this link.</p>"},{"location":"Speedster/overview/","title":"Overview","text":"<p><code>Speedster</code> is an open-source module designed to accelerate AI inference in just a few lines of code. The library allows you to seamlessy modulate the inference performances of your AI models in terms of latency, throughput, model size, accuracy, cost and automatically applies the best set of optimization techniques along the software to hardware stack to meet your targets.</p> <p><code>Speedster</code> makes it easy to combine optimization techniques across the whole software to hardware stack, delivering best in class speed-ups. If you like the idea, give us a star to support the project\u00a0\u2b50</p> <p></p> <p>The core\u00a0<code>Speedster</code>\u00a0workflow consists of 3 steps:</p> <ul> <li> Select: input your model in your preferred DL framework and express your preferences regarding:<ul> <li>Accuracy loss: do you want to trade off a little accuracy for much higher performance?</li> <li>Optimization time: stellar accelerations can be time-consuming. Can you wait, or do you need an instant answer?</li> </ul> </li> <li> Search: the library automatically tests every combination of optimization techniques across the software-to-hardware stack (sparsity, quantization, compilers, etc.) that is compatible with your needs and local hardware.</li> <li> Serve: finally, <code>Speedster</code> chooses the best configuration of optimization techniques and returns an accelerated version of your model in the DL framework of your choice (just on steroids\u00a0\ud83d\ude80).</li> </ul> <p>Now you are ready to start accelerating your models, visit the Installation section to start right away!</p>"},{"location":"Speedster/telemetry/","title":"Telemetry","text":"<p><code>Speedster</code> is a young and rapidly evolving open-source project. There is plenty of room for improvement for Speedster to make your model achieve the very best performance on your hardware... and you may still find some bugs in the code \ud83e\udeb2</p> <p>Contributions to this OSS project are warmly welcomed \ud83e\udd17. We encourage you to check out the Contribution guidelines to understand how you can become an active contributor of the source code.</p>"},{"location":"Speedster/telemetry/#sharing-feedback-to-improve-speedster","title":"Sharing feedback to improve Speedster","text":"<p>Open source is a unique resource for sharing knowledge and building great projects collaboratively with the OSS community. To support the continued development, upon installation of Speedster you could share the information strictly necessary to improve the performance of this open-source project and facilitate bug detection and fixing.</p> <p>More specifically, you will foster project enhancement by sharing details of the optimization techniques used with Speedster and the performance achieved on your model and hardware.</p> <p>Which data do we collect?</p> <p>We make sure to collect as little data as possible to improve the open-source project:</p> <ul> <li>basic information about the environment</li> <li>basic information about the optimization</li> </ul> <p>Please find below an example of telemetry collection:</p> <pre><code>{\n\"nebullvm_version\": \"0.6.0\",\n\"app_version\": \"0.0.1\",\n\"model_id\": \"e33a1bbf-fcfd-4f5a-81c9-a9154c7e9343_-7088971112344091114\",\n\"model_metadata\": {\n\"model_name\": \"ResNet\",\n\"model_size\": \"102.23 MB\",\n\"framework\": \"torch\"\n},\n\"hardware_setup\": {\n\"cpu\": \"Apple M1 Pro\",\n\"operative_system\": \"Darwin\",\n\"ram\": \"17.18 GB\"\n},\n\"optimizations\": [\n{\n\"compiler\": \"torch\",\n\"technique\": \"original\",\n\"latency\": 0.03\n},\n{\n\"compiler\": \"NUMPY_onnxruntime\",\n\"technique\": \"none\",\n\"latency\": 0.01\n}\n],\n\"ip_address\": \"1.1.1.1\"\n}\n</code></pre> <p>How to opt-out?</p> <p>You can simply opt-out from telemetry collection by setting the environment variable <code>SPEEDSTER_DISABLE_TELEMETRY to 1</code>.</p> <p>Should I opt out?</p> <p>Being open-source, we have very limited visibility into the use of the tool unless someone actively contacts us or opens an issue on GitHub.</p> <p>We would appreciate it if you would maintain telemetry, as it helps us improve the source code. In fact, it brings increasing value to the project and helps us to better prioritize feature development.</p> <p>We understand that you may still prefer not to share telemetry data and we respect that desire. Please follow the steps above to disable data collection.</p>"},{"location":"Speedster/getting_started/diffusers_getting_started/","title":"Getting started with Stable Diffusion optimization","text":"<p>In this section, we will learn about the 4 main steps needed to optimize Stable Diffusion models from the <code>Diffusers</code> library:</p> <ol> <li>Environment Setup</li> <li>Input your model and data</li> <li>Run the optimization</li> <li>Save your optimized model</li> <li>Load and run your optimized model in production</li> </ol>"},{"location":"Speedster/getting_started/diffusers_getting_started/#1-environment-setup-gpu-only","title":"1) Environment Setup (GPU only)","text":"<p>In order to optimize a Stable Diffusion model, you have to ensure that your environment is correctly set up according to these requirements: <code>CUDA&gt;=12.0</code> and <code>tensorrt&gt;=8.6.0</code>.</p> <p>From TensorRT 8.6, all the tensorrt pre-built wheels released by nvidia support only <code>CUDA&gt;=12.0</code>. Speedster will install <code>tensorrt&gt;=8.6.0</code> automatically in the auto-installer only if it detects CUDA&gt;=12.0, otherwise it will install <code>tensorrt==8.5.3.1</code>. In that case, you will have to upgrade your CUDA version and then to upgarde tensorrt to 8.6.0 or above.</p> <p>There should be a way to run TensorRT 8.6 also with CUDA 11, but it requires installing TensorRT in a different way, you can check this issue: https://github.com/NVIDIA/TensorRT/issues/2773. Otherwise, we highly suggest to just upgrade to CUDA 12.</p> <p>You can check your CUDA version with the following command:</p> <pre><code>nvidia-smi\n</code></pre> <p>If you have CUDA&lt;12.0, you can upgrade it at this link: https://developer.nvidia.com/cuda-downloads</p> <p>You can check your TensorRT version with the following command:</p> <pre><code>python -c \"import tensorrt; print(tensorrt.__version__)\"\n</code></pre> <p>If you have an older version, after ensuring you have <code>CUDA&gt;=12.0</code> installed, you can upgrade your TensorRT version by running: <pre><code>pip install -U tensorrt\n</code></pre></p>"},{"location":"Speedster/getting_started/diffusers_getting_started/#2-input-model-and-data","title":"2) Input model and data","text":"<p>Info</p> <p>In order to optimize a model with <code>Speedster</code>, first you should input the model you want to optimize and load some sample data that will be needed to test the optimization performances (latency, throughput, accuracy loss, etc). </p> <p>For Stable Diffusion models Speedster expects the input data to be a list of sentences: <code>List[str]</code></p> <pre><code>import torch\nfrom speedster import optimize_model\nfrom diffusers import StableDiffusionPipeline\n# Load Stable Diffusion 1.4 as example\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif device == \"cuda\":\n# On GPU we load by default the model in half precision, because it's faster and lighter.\npipe = StableDiffusionPipeline.from_pretrained(model_id, revision='fp16', torch_dtype=torch.float16)\nelse:\npipe = StableDiffusionPipeline.from_pretrained(model_id)\n# Create some example input data\ninput_data = [\n\"a photo of an astronaut riding a horse on mars\",\n\"a monkey eating a banana in a forest\",\n\"white car on a road surrounded by palm trees\",\n\"a fridge full of bottles of beer\",\n\"madara uchiha throwing asteroids against people\"\n]\n</code></pre> <p>Now your input model and data are ready, you can move on to Run the optimization section \ud83d\ude80.</p>"},{"location":"Speedster/getting_started/diffusers_getting_started/#3-run-the-optimization","title":"3) Run the optimization","text":"<p>Once the <code>model</code> and <code>input_data</code> have been defined, everything is ready to use Speedster's <code>optimize_model</code> function to optimize your model. </p> <p>The function takes the following arguments as inputs:</p> <ul> <li><code>model</code>: model to be optimized in your preferred framework (A Diffusers pipe in this case)</li> <li><code>input_data</code>: sample data needed to test the optimization performances (latency, throughput, accuracy loss, etc)</li> <li><code>optimization_time</code>: if \"constrained\" mode, <code>Speedster</code> takes advantage only of compilers and precision reduction techniques, such as quantization. \"unconstrained\" optimization_time allows it to exploit more time-consuming techniques, such as pruning and distillation </li> <li><code>metric_drop_ths</code>: maximum drop in your preferred accuracy metric that you are willing to trade to gain in acceleration</li> </ul> <p>and returns the accelerated version of your model \ud83d\ude80.</p> <pre><code>from speedster import optimize_model\n# Run Speedster optimization\noptimized_model = optimize_model(\npipe, \ninput_data=input_data, \noptimization_time=\"unconstrained\",\nmetric_drop_ths=0.05\n)\n</code></pre> <p>Internally, <code>Speedster</code> tries to use all the compilers and optimization techniques at its disposal along the software to hardware stack to optimize the model. From these, it will choose the ones with the lowest latency on the specific hardware.</p> <p>At the end of the optimization, you are going to see the results in a summary table like the following:</p> <p></p> <p>If the speedup you obtained is good enough for your application, you can move to the Save your optimized model section to save your model and use it in production.</p> <p>If you want to squeeze out even more acceleration out of the model, please see the <code>optimize_model</code> API section. Consider if in your application you can trade off a little accuracy for much higher performance and use the <code>metric</code>, <code>metric_drop_ths</code> and <code>optimization_time</code> arguments accordingly.</p>"},{"location":"Speedster/getting_started/diffusers_getting_started/#4-save-your-optimized-model","title":"4) Save your optimized model","text":"<p>After accelerating the model, it can be saved using the <code>save_model</code> function:</p> <pre><code>from speedster import save_model\nsave_model(optimized_model, \"model_save_path\")\n</code></pre> <p>Now you are all set to use your optimized model in production. To explore how to do it, see the Load and run your optimized model in production section.</p>"},{"location":"Speedster/getting_started/diffusers_getting_started/#5-load-and-run-your-optimized-model-in-production","title":"5) Load and run your optimized model in production","text":"<p>Once the optimized model has been saved,  it can be loaded with the <code>load_model</code> function: <pre><code>from speedster import load_model\noptimized_model = load_model(\"model_save_path\", pipe=pipe)\n</code></pre></p> <p>In this case we must provide also the original pipe as argument to the load_function, Speedster will automatically load the optimized model and replace the original UNet inside the pipe.</p> <p>The optimized model can be used for accelerated inference in the same way as the original model:</p> <pre><code># Use the accelerated version of your Stable Diffusion model in production\noutput = optimized_model(test_prompt).images[0]\n</code></pre> <p>Info</p> <p>The first 1-2 inferences could be a bit slower than expected because some compilers still perform some optimizations during the first iterations. After this warm-up time, the next ones will be faster than ever.</p> <p>If you want to know more about how to squeeze out more performances from your models, please visit the Advanced options section.</p>"},{"location":"Speedster/getting_started/hf_getting_started/","title":"Getting started with HuggingFace optimization","text":"<p>In this section, we will learn about the 4 main steps needed to optimize your \ud83e\udd17 HuggingFace models:</p> <ol> <li>Input your model and data</li> <li>Run the optimization</li> <li>Save your optimized model</li> <li>Load and run your optimized model in production</li> </ol>"},{"location":"Speedster/getting_started/hf_getting_started/#1-input-model-and-data","title":"1) Input model and data","text":"<p>Info</p> <p>In order to optimize a model with <code>Speedster</code>, first you should input the model you want to optimize and load some sample data that will be needed to test the optimization performances (latency, throughput, accuracy loss, etc). </p> <p>For HuggingFace models we support different types of input data depending on the architecture of your input model.</p> <ul> <li> <p> For Decoder-only or Encoder-only architectures (Bert, GPT, etc), we support:</p> <ul> <li>Dictionary</li> <li>String</li> </ul> </li> <li> <p> For Encoder-Decoder architectures (T5 etc), we support: </p> <ul> <li>Dictionary</li> </ul> </li> </ul> Decoder-only or Encoder-only (Bert, GPT, etc)Encoder-Decoder architectures (T5 etc) <p>Input as Dictionary</p> <p><pre><code>from transformers import AlbertModel, AlbertTokenizer\n# Load Albert as example\nmodel = AlbertModel.from_pretrained(\"albert-base-v1\")\ntokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v1\")\n# Case 1: dictionary input format\ntext = \"This is an example text for the huggingface model.\"\ninput_dict = tokenizer(text, return_tensors=\"pt\")\ninput_data = [input_dict for _ in range(100)]\n</code></pre> Now your input model and data are ready, you can move on to Run the optimization section \ud83d\ude80.</p> <p>Input as String</p> <p>In the string case, the HuggingFace tokenizer must be given as input to the <code>optimize_model</code> in addition to the <code>input_data</code>, and the arguments for the tokenizer can be passed using the param <code>tokenizer_args</code>.</p> <p><pre><code>from transformers import AlbertModel, AlbertTokenizer\n# Load Albert as example\nmodel = AlbertModel.from_pretrained(\"albert-base-v1\")\ntokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v1\")\n# Case 2: strings input format\ninput_data = [\n\"This is a test.\",\n\"Hi my name is John.\",\n\"The cat is on the table.\",\n]\ntokenizer_args = dict(\nreturn_tensors=\"pt\",\npadding=\"longest\",\ntruncation=True,\n)\n</code></pre> Now your input model and data are ready, you can move on to Run the optimization section \ud83d\ude80.</p> <p>For encoder-decoder architectures we support only <code>input_data</code> as Dictionary: <pre><code>from transformers import T5Tokenizer, T5ForConditionalGeneration\n# Load T5 as example\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\") \n# Case 1: dictionary input format\nquestion = \"What's the meaning of life?\"\nanswer = \"The answer is:\"\ninput_dict = tokenizer(question, return_tensors=\"pt\")\ninput_dict[\"decoder_input_ids\"] = tokenizer(answer, return_tensors=\"pt\").input_ids\ninput_data = [input_dict for _ in range(100)]\n</code></pre> Now your input model and data are ready, you can move on to Run the optimization section \ud83d\ude80.</p>"},{"location":"Speedster/getting_started/hf_getting_started/#2-run-the-optimization","title":"2) Run the optimization","text":"<p>Once the <code>model</code> and <code>input_data</code> have been defined, everything is ready to use Speedster's <code>optimize_model</code> function to optimize your model. </p> <p>The function takes the following arguments as inputs:</p> <ul> <li><code>model</code>: model to be optimized in your preferred framework (HuggingFace in this case)</li> <li><code>input_data</code>: sample data needed to test the optimization performances (latency, throughput, accuracy loss, etc)</li> <li><code>optimization_time</code>: if \"constrained\" mode, <code>Speedster</code> takes advantage only of compilers and precision reduction techniques, such as quantization. \"unconstrained\" optimization_time allows it to exploit more time-consuming techniques, such as pruning and distillation </li> <li><code>metric_drop_ths</code>: maximum drop in your preferred accuracy metric that you are willing to trade to gain in acceleration</li> </ul> <p>and returns the accelerated version of your model \ud83d\ude80.</p> <p>Depending on the format of your <code>input_data</code>, the <code>optimize_model</code> is as follows:</p> Input as DictionaryInput as String <pre><code>from speedster import optimize_model\n# Run Speedster optimization\noptimized_model = optimize_model(\nmodel, \ninput_data=input_data, \noptimization_time=\"constrained\",\nmetric_drop_ths=0.05\n)\n</code></pre> <pre><code>from speedster import optimize_model\n# Run Speedster optimization\noptimized_model = optimize_model(\nmodel, \ninput_data=input_data, \noptimization_time=\"constrained\", \nmetric_drop_ths=0.05,\ntokenizer=tokenizer,\ntokenizer_args={\"return_tensors\": \"pt\"}\n)\n</code></pre> <p>Internally, <code>Speedster</code> tries to use all the compilers and optimization techniques at its disposal along the software to hardware stack to optimize the model. From these, it will choose the ones with the lowest latency on the specific hardware.</p> <p>At the end of the optimization, you are going to see the results in a summary table like the following:</p> <p></p> <p>If the speedup you obtained is good enough for your application, you can move to the Save your optimized model section to save your model and use it in production.</p> <p>If you want to squeeze out even more acceleration out of the model, please see the <code>optimize_model</code> API section. Consider if in your application you can trade off a little accuracy for much higher performance and use the <code>metric</code>, <code>metric_drop_ths</code> and <code>optimization_time</code> arguments accordingly.</p>"},{"location":"Speedster/getting_started/hf_getting_started/#3-save-your-optimized-model","title":"3) Save your optimized model","text":"<p>After accelerating the model, it can be saved using the <code>save_model</code> function:</p> <pre><code>from speedster import save_model\nsave_model(optimized_model, \"model_save_path\")\n</code></pre> <p>Now you are all set to use your optimized model in production. To explore how to do it, see the Load and run your optimized model in production section.</p>"},{"location":"Speedster/getting_started/hf_getting_started/#4-load-and-run-your-optimized-model-in-production","title":"4) Load and run your optimized model in production","text":"<p>Once the optimized model has been saved,  it can be loaded with the <code>load_model</code> function: <pre><code>from speedster import load_model\noptimized_model = load_model(\"model_save_path\")\n</code></pre></p> <p>The optimized model can be used for accelerated inference in the same way as the original model:</p> <pre><code># Use the accelerated version of your HuggingFace model in production\noutput = optimized_model(**input_sample)\n</code></pre> <p>Info</p> <p>The first 1-2 inferences could be a bit slower than expected because some compilers still perform some optimizations during the first iterations. After this warm-up time, the next ones will be faster than ever.</p> <p>If you want to know more about how to squeeze out more performances from your models, please visit the Advanced options section.</p>"},{"location":"Speedster/getting_started/onnx_getting_started/","title":"Getting started with ONNX optimization","text":"<p>In this section, we will learn about the 4 main steps needed to optimize your ONNX models:</p> <ol> <li>Input your model and data</li> <li>Run the optimization</li> <li>Save your optimized model</li> <li>Load and run your optimized model in production</li> </ol>"},{"location":"Speedster/getting_started/onnx_getting_started/#1-input-model-and-data","title":"1) Input model and data","text":"<p>Info</p> <p>In order to optimize a model with <code>Speedster</code>, first you should input the model you want to optimize and load some sample data that will be needed to test the optimization performances (latency, throughput, accuracy loss, etc). </p> <pre><code>import numpy as np\n# Load a resnet as example\n# Model was downloaded from here: \n# https://github.com/onnx/models/tree/main/vision/classification/resnet\nmodel = \"resnet50-v1-12.onnx\"\n# Provide input data for the model    \ninput_data = [((np.random.randn(1, 3, 224, 224).astype(np.float32), ), np.array([0])) for _ in range(100)]\n</code></pre> <p>Now your input model and data are ready, you can move on to Run the optimization section \ud83d\ude80.</p>"},{"location":"Speedster/getting_started/onnx_getting_started/#2-run-the-optimization","title":"2) Run the optimization","text":"<p>Once the <code>model</code> and <code>input_data</code> have been defined, everything is ready to use Speedster's <code>optimize_model</code> function to optimize your model. </p> <p>The function takes the following arguments as inputs:</p> <ul> <li><code>model</code>: model to be optimized in your preferred framework (ONNX in this case)</li> <li><code>input_data</code>: sample data needed to test the optimization performances (latency, throughput, accuracy loss, etc)</li> <li><code>optimization_time</code>: if \"constrained\" mode, <code>Speedster</code> takes advantage only of compilers and precision reduction techniques, such as quantization. \"unconstrained\" optimization_time allows it to exploit more time-consuming techniques, such as pruning and distillation </li> <li><code>metric_drop_ths</code>: maximum drop in your preferred accuracy metric that you are willing to trade to gain in acceleration</li> </ul> <p>and returns the accelerated version of your model \ud83d\ude80.</p> <pre><code>from speedster import optimize_model\n# Run Speedster optimization\noptimized_model = optimize_model(\nmodel, \ninput_data=input_data, \noptimization_time=\"constrained\",\nmetric_drop_ths=0.05\n)\n</code></pre> <p>Internally, <code>Speedster</code> tries to use all the compilers and optimization techniques at its disposal along the software to hardware stack to optimize the model. From these, it will choose the ones with the lowest latency on the specific hardware.</p> <p>At the end of the optimization, you are going to see the results in a summary table like the following:</p> <p></p> <p>If the speedup you obtained is good enough for your application, you can move to the Save your optimized model section to save your model and use it in production.</p> <p>If you want to squeeze out even more acceleration out of the model, please see the <code>optimize_model</code> API section. Consider if in your application you can trade off a little accuracy for much higher performance and use the <code>metric</code>, <code>metric_drop_ths</code> and <code>optimization_time</code> arguments accordingly.</p>"},{"location":"Speedster/getting_started/onnx_getting_started/#3-save-your-optimized-model","title":"3) Save your optimized model","text":"<p>After accelerating the model, it can be saved using the <code>save_model</code> function:</p> <pre><code>from speedster import save_model\nsave_model(optimized_model, \"model_save_path\")\n</code></pre> <p>Now you are all set to use your optimized model in production. To explore how to do it, see the Load and run your optimized model in production section.</p>"},{"location":"Speedster/getting_started/onnx_getting_started/#4-load-and-run-your-optimized-model-in-production","title":"4) Load and run your optimized model in production","text":"<p>Once the optimized model has been saved,  it can be loaded with the <code>load_model</code> function: <pre><code>from speedster import load_model\noptimized_model = load_model(\"model_save_path\")\n</code></pre></p> <p>The optimized model can be used for accelerated inference in the same way as the original model:</p> <pre><code># Use the accelerated version of your ONNX model in production\noutput = optimized_model(input_sample)\n</code></pre> <p>Info</p> <p>The first 1-2 inferences could be a bit slower than expected because some compilers still perform some optimizations during the first iterations. After this warm-up time, the next ones will be faster than ever.</p> <p>If you want to know more about how to squeeze out more performances from your models, please visit the Advanced options section.</p>"},{"location":"Speedster/getting_started/pytorch_getting_started/","title":"Getting started with PyTorch optimization","text":"<p>In this section, we will learn about the 4 main steps needed to optimize PyTorch models:</p> <ol> <li>Input your model and data</li> <li>Run the optimization</li> <li>Save your optimized model</li> <li>Load and run your optimized model in production</li> </ol>"},{"location":"Speedster/getting_started/pytorch_getting_started/#1-input-model-and-data","title":"1) Input model and data","text":"<p>Info</p> <p>In order to optimize a model with <code>Speedster</code>, first you should input the model you want to optimize and load some sample data that will be needed to test the optimization performances (latency, throughput, accuracy loss, etc). </p> <p>For PyTorch models we support two types of input data:</p> <ul> <li>Custom data format</li> <li>PyTorch DataLoader</li> </ul> Custom Data FormatPyTorch DataLoader <p>Input data is a <code>List[Tuple[Tuple[tensor, ...], tensor]]</code></p> <ul> <li>Each element of the list is a tuple, which represents a batch of the dataset.</li> <li>In each tuple, the first element is another tuple containing a value for each input tensor of the model, while the second element is a tensor containing the labels of that batch of data. The label is optional, so it can be omitted.</li> </ul> <pre><code>import torch\nimport torchvision.models as models\n# Load a resnet as example\nmodel = models.resnet50()\n# Provide input data for the model    \ninput_data = [((torch.randn(1, 3, 256, 256), ), torch.tensor([0])) for _ in range(100)]\n</code></pre> <p>See below further examples with custom format: <pre><code># Dataset for a model that takes 1 input, containing 100 batches of data with bs=1 with labels\ninput_data = [((torch.randn(1, 3, 256, 256), ), torch.tensor([0])) for _ in range(100)]\n# Dataset for a model that takes 2 inputs, containing 100 batches of data with bs=5 with labels\ninput_data = [((torch.randn(5, 3, 256, 256), torch.randn(5, 3, 256, 256), ), torch.tensor([0, 1, 0, 1, 1])) for _ in range(100)]\n# Dataset for a model that takes 1 input, containing 100 batches of data with bs=1 without labels\ninput_data = [((torch.randn(1, 3, 256, 256), ), ) for _ in range(100)]\n</code></pre></p> <p>Now your input model and data are ready, you can move on to Run the optimization section \ud83d\ude80.</p> <p>We support the following DataLoader types:</p> <ul> <li>Tensor only</li> <li>Tensor and labels</li> </ul> <p>For models with multiple inputs, we support the following types:</p> <ul> <li>input_1, input_2, ..., input_n, label</li> <li>(input_1, input_2, ..., input_n), label</li> </ul> <pre><code>import torch\nimport torchvision.models as models\n# Load a resnet as example\nmodel = models.resnet50()\n# Use your PyTorch DataLoader in any of the standard format\ninput_data = &lt;insert your PyTorch DataLoader here&gt;\n</code></pre> <p>Now your input <code>model</code> and <code>input_data</code> are ready, you can move on to the Run the optimization section.</p>"},{"location":"Speedster/getting_started/pytorch_getting_started/#2-run-the-optimization","title":"2) Run the optimization","text":"<p>Once the <code>model</code> and <code>input_data</code> have been defined, everything is ready to use Speedster's <code>optimize_model</code> function to optimize your model. </p> <p>The function takes the following arguments as inputs:</p> <ul> <li><code>model</code>: model to be optimized in your preferred framework (PyTorch in this case)</li> <li><code>input_data</code>: sample data needed to test the optimization performances (latency, throughput, accuracy loss, etc)</li> <li><code>optimization_time</code>: if \"constrained\" mode, <code>Speedster</code> takes advantage only of compilers and precision reduction techniques, such as quantization. \"unconstrained\" optimization_time allows it to exploit more time-consuming techniques, such as pruning and distillation </li> <li><code>metric_drop_ths</code>: maximum drop in your preferred accuracy metric that you are willing to trade to gain in acceleration</li> </ul> <p>and returns the accelerated version of your model \ud83d\ude80.</p> <pre><code>from speedster import optimize_model\n# Run Speedster optimization\noptimized_model = optimize_model(\nmodel, \ninput_data=input_data, \noptimization_time=\"constrained\",\nmetric_drop_ths=0.05\n)\n</code></pre> <p>Internally, <code>Speedster</code> tries to use all the compilers and optimization techniques at its disposal along the software to hardware stack to optimize the model. From these, it will choose the ones with the lowest latency on the specific hardware.</p> <p>At the end of the optimization, you are going to see the results in a summary table like the following:</p> <p></p> <p>If the speedup you obtained is good enough for your application, you can move to the Save your optimized model section to save your model and use it in production.</p> <p>If you want to squeeze out even more acceleration out of the model, please see the <code>optimize_model</code> API section. Consider if in your application you can trade off a little accuracy for much higher performance and use the <code>metric</code>, <code>metric_drop_ths</code> and <code>optimization_time</code> arguments accordingly.</p>"},{"location":"Speedster/getting_started/pytorch_getting_started/#3-save-your-optimized-model","title":"3) Save your optimized model","text":"<p>After accelerating the model, it can be saved using the <code>save_model</code> function:</p> <pre><code>from speedster import save_model\nsave_model(optimized_model, \"model_save_path\")\n</code></pre> <p>Now you are all set to use your optimized model in production. To explore how to do it, see the Load and run your optimized model in production section.</p>"},{"location":"Speedster/getting_started/pytorch_getting_started/#4-load-and-run-your-optimized-model-in-production","title":"4) Load and run your optimized model in production","text":"<p>Once the optimized model has been saved,  it can be loaded with the <code>load_model</code> function: <pre><code>from speedster import load_model\noptimized_model = load_model(\"model_save_path\")\n</code></pre></p> <p>The optimized model can be used for accelerated inference in the same way as the original model:</p> <pre><code># Use the accelerated version of your PyTorch model in production\noutput = optimized_model(input_sample)\n</code></pre> <p>Info</p> <p>The first 1-2 inferences could be a bit slower than expected because some compilers still perform some optimizations during the first iterations. After this warm-up time, the next ones will be faster than ever.</p> <p>If you want to know more about how to squeeze out more performances from your models, please visit the Advanced options section.</p>"},{"location":"Speedster/getting_started/tf_getting_started/","title":"Getting started with TensorFlow optimization","text":"<p>In this section, we will learn about the 4 main steps needed to optimize TensorFlow models:</p> <ol> <li>Input your model and data</li> <li>Run the optimization</li> <li>Save your optimized model</li> <li>Load and run your optimized model in production</li> </ol>"},{"location":"Speedster/getting_started/tf_getting_started/#1-input-model-and-data","title":"1) Input model and data","text":"<p>Info</p> <p>In order to optimize a model with <code>Speedster</code>, first you should input the model you want to optimize and load some sample data that will be needed to test the optimization performances (latency, throughput, accuracy loss, etc). </p> <p>For TensorFlow models we support two types of input data:</p> <ul> <li>Custom data format</li> <li>TensorFlow DataLoader</li> </ul> Custom Data FormatTensorFlow DataLoader <p>Input data is a <code>List[Tuple[Tuple[tensor, ...], tensor]]</code></p> <ul> <li>Each element of the list is a tuple, which represents a batch of the dataset.</li> <li>In each tuple, the first element is another tuple containing a value for each input tensor of the model, while the second element is a tensor containing the labels of that batch of data. The label is optional, so it can be omitted.</li> </ul> <pre><code>import tensorflow as tf\nfrom tensorflow.keras.applications.resnet50 import ResNet50\n# Load a resnet as example\nmodel = ResNet50()\n# Provide input data for the model    \ninput_data = [((tf.random.normal([1, 224, 224, 3]),), tf.constant([0])) for _ in range(100)]\n</code></pre> <p>Now your input model and data are ready, you can move on to Run the optimization section \ud83d\ude80.</p> <p>We support the following DataLoader types:</p> <ul> <li>Tensor only</li> <li>Tensor and labels</li> </ul> <p>For models with multiple inputs, we support the following types:</p> <ul> <li>input_1, input_2, ..., input_n, label</li> <li>(input_1, input_2, ..., input_n), label</li> </ul> <pre><code>import torch\nimport torchvision.models as models\n# Load a resnet as example\nmodel = models.resnet50()\n# Use your TensorFlow DataLoader in any of the standard format\ninput_data = &lt;insert your TensorFlow DataLoader here&gt;\n</code></pre> <p>Now your input <code>model</code> and <code>input_data</code> are ready, you can move on to the Run the optimization section.</p>"},{"location":"Speedster/getting_started/tf_getting_started/#2-run-the-optimization","title":"2) Run the optimization","text":"<p>Once the <code>model</code> and <code>input_data</code> have been defined, everything is ready to use Speedster's <code>optimize_model</code> function to optimize your model. </p> <p>The function takes the following arguments as inputs:</p> <ul> <li><code>model</code>: model to be optimized in your preferred framework (TensorFlow in this case)</li> <li><code>input_data</code>: sample data needed to test the optimization performances (latency, throughput, accuracy loss, etc)</li> <li><code>optimization_time</code>: if \"constrained\" mode, <code>Speedster</code> takes advantage only of compilers and precision reduction techniques, such as quantization. \"unconstrained\" optimization_time allows it to exploit more time-consuming techniques, such as pruning and distillation </li> <li><code>metric_drop_ths</code>: maximum drop in your preferred accuracy metric that you are willing to trade to gain in acceleration</li> </ul> <p>and returns the accelerated version of your model \ud83d\ude80.</p> <pre><code>from speedster import optimize_model\n# Run Speedster optimization\noptimized_model = optimize_model(\nmodel, \ninput_data=input_data, \noptimization_time=\"constrained\",\nmetric_drop_ths=0.05\n)\n</code></pre> <p>Internally, <code>Speedster</code> tries to use all the compilers and optimization techniques at its disposal along the software to hardware stack to optimize the model. From these, it will choose the ones with the lowest latency on the specific hardware.</p> <p>At the end of the optimization, you are going to see the results in a summary table like the following:</p> <p></p> <p>If the speedup you obtained is good enough for your application, you can move to the Save your optimized model section to save your model and use it in production.</p> <p>If you want to squeeze out even more acceleration out of the model, please see the <code>optimize_model</code> API section. Consider if in your application you can trade off a little accuracy for much higher performance and use the <code>metric</code>, <code>metric_drop_ths</code> and <code>optimization_time</code> arguments accordingly.</p>"},{"location":"Speedster/getting_started/tf_getting_started/#3-save-your-optimized-model","title":"3) Save your optimized model","text":"<p>After accelerating the model, it can be saved using the <code>save_model</code> function:</p> <pre><code>from speedster import save_model\nsave_model(optimized_model, \"model_save_path\")\n</code></pre> <p>Now you are all set to use your optimized model in production. To explore how to do it, see the Load and run your optimized model in production section.</p>"},{"location":"Speedster/getting_started/tf_getting_started/#4-load-and-run-your-optimized-model-in-production","title":"4) Load and run your optimized model in production","text":"<p>Once the optimized model has been saved,  it can be loaded with the <code>load_model</code> function:</p> <pre><code>from speedster import load_model\noptimized_model = load_model(\"model_save_path\")\n</code></pre> <p>The optimized model can be used for accelerated inference in the same way as the original model:</p> <pre><code># Use the accelerated version of your TensorFlow model in production\noutput = optimized_model(input_sample)\n</code></pre> <p>Info</p> <p>The first 1-2 inferences could be a bit slower than expected because some compilers still perform some optimizations during the first iterations. After this warm-up time, the next ones will be faster than ever.</p> <p>If you want to know more about how to squeeze out more performances from your models, please visit the Advanced options section.</p>"},{"location":"nos/installation/","title":"Installation","text":"<p>Warning</p> <p>Before proceeding with <code>nos</code> installation, please make sure to meet the requirements  described in the Prerequisites page.</p> <p>You can install <code>nos</code> using Helm 3 (recommended). You can find all the available configuration values in the Chart documentation.</p> <pre><code>helm install oci://ghcr.io/nebuly-ai/helm-charts/nos \\\n--version 0.1.2 \\\n--namespace nebuly-nos \\\n--generate-name \\\n--create-namespace\n</code></pre> <p>Alternatively, you can use Kustomize by cloning the repository and running <code>make deploy</code>.</p>"},{"location":"nos/installation/#next-steps","title":"Next steps","text":"<ul> <li>Getting started with Dynamic MIG Partitioning</li> <li>Getting started with Dynamic MPS Partitioning</li> <li>Getting started with Elastic Resource Quotas</li> </ul>"},{"location":"nos/overview/","title":"Overview","text":"<p><code>nos</code> is the open-source module for running AI workloads on Kubernetes in an optimized way, increasing GPU utilization, cutting down infrastructure costs and improving workloads performance.</p> <p>Currently, the available features are:</p> <ul> <li> <p>Dynamic GPU partitioning: allow to schedule Pods requesting fractions of GPU. GPU partitioning is performed automatically in real-time based on the Pods pending and running in the cluster, so that Pods can request only the resources that are strictly necessary and GPUs are always fully utilized.</p> </li> <li> <p>Elastic Resource Quota management: increase the number of Pods running on the cluster by allowing namespaces to borrow quotas of reserved resources from other namespaces as long as they are not using them.</p> </li> </ul> <p></p>"},{"location":"nos/prerequisites/","title":"Prerequisites","text":"<ol> <li>Kubernetes version 1.23 or newer</li> <li>GPU Support must be enabled</li> <li>Nebuly's device plugin (required only if using MPS partitioning)</li> <li>Cert Manager (optional, but recommended)</li> </ol>"},{"location":"nos/prerequisites/#enable-gpu-support","title":"Enable GPU support","text":"<p>Before installing <code>nos</code>, you must enable GPU support in your Kubernetes cluster.</p> <p>There are two ways to do this. One option is to manually install the required components individually,while the other consists in installing only the NVIDIA GPU Operator, which automatically installs all the necessary components for you. See below for more information on these two installation methods.</p> <p>We recommended enabling GPU support using the NVIDIA GPU Operator (option 1).</p>"},{"location":"nos/prerequisites/#option-1-nvidia-gpu-operator","title":"Option 1 - NVIDIA GPU Operator","text":"<p>You can install the NVIDIA GPU Operator as follows:</p> <pre><code>helm install --wait --generate-name \\\n-n gpu-operator --create-namespace \\\nnvidia/gpu-operator --version v22.9.0 \\\n--set driver.enabled=true \\\n--set migManager.enabled=false \\\n--set mig.strategy=mixed \\\n--set toolkit.enabled=true\n</code></pre> <p>Note that the GPU Operator will automatically install a recent version of NVIDIA Drivers and CUDA on all the GPU-enabled nodes of your cluster, so you don't have to manually install them.</p> <p>For further information you can refer to the NVIDIA GPU Operator Documentation.</p>"},{"location":"nos/prerequisites/#option-2-manual-installation","title":"Option 2 - Manual installation","text":"<p>Warning</p> <p>If you want to enable MPS Dynamic Partitioning, make sure you have a version of CUDA 11.5 or newer installed, as this is the minimum version that supports GPU memory limits in MPS.</p> <p>To enble GPU support in your cluster, you first need to install NVIDIA Drivers and the NVIDIA Container Toolkit on all the nodes of your cluster with a GPU.</p> <p>After installing the NVIDIA Drivers and the Container Toolkit on your nodes, you need to install the following Kubernetes components:</p> <ul> <li>NVIDIA GPU Feature Discovery</li> <li>NVIDIA Device Plugin</li> </ul> <p>Please note that the configuration parameter <code>migStrategy</code> must be set to <code>mixed</code> (you can do that with <code>--set migStrategy=mixed</code> if you are using Helm).</p>"},{"location":"nos/prerequisites/#install-nebulys-device-plugin","title":"Install Nebuly's device plugin","text":"<p>Info</p> <p>Nebuly's device plugin is required only if you want to use dynamic MPS partitioning. If you don't plan to use MPS partitioning, you can then skip this installation step.</p> <p>You can install Nebuly's device plugin using Helm as follows:</p> <pre><code>helm install oci://ghcr.io/nebuly-ai/helm-charts/nvidia-device-plugin \\\n--version 0.13.0 \\\n--generate-name \\\n-n nebuly-nvidia \\\n--create-namespace\n</code></pre> <p>Nebuly's device plugin runs only on nodes labelled with <code>nos.nebuly.com/gpu-partitioning=mps</code>.</p> <p>If you already have the NVIDIA device plugin installed on your cluster, you need to ensure that only one instance of the device plugin is running on each GPU node (either Nebuly's or NVIDIA's). One way to do that is to add an affinity rule to the NVIDIA device plugin Daemonset so that it doesn't run on any node that has MPS enabled:</p> <pre><code>affinity:\nnodeAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\nnodeSelectorTerms:\n- matchExpressions:\n- key: nos.nebuly.com/gpu-partitioning\noperator: NotIn\nvalues:\n- mps\n</code></pre> <p>For further information you can refer to Nebuly's device plugin documentation.</p>"},{"location":"nos/telemetry/","title":"Sharing feedback to improve <code>nos</code>","text":"<p>Open source is a unique resource for sharing knowledge and building great projects collaboratively with the OSS community. To support the development of <code>nos</code>, during the installation of you could share the information strictly necessary to improve the features of this open-source project and facilitate bug detection and fixing.</p> <p>More specifically, you will foster project enhancement by sharing details about the setup and configuration of the environment where you are installing <code>nos</code> and its components.</p> <p>Which data do we collect?</p> <p>We make sure to collect as little data as possible to improve the open-source project:</p> <ul> <li>basic information about the Kubernetes cluster<ul> <li>Kubernetes version</li> <li>Number of nodes</li> </ul> </li> <li>basic information about each node of the cluster<ul> <li>Kubelet version</li> <li>Operating system</li> <li>Container runtime</li> <li>Node resources</li> <li>Labels from the NVIDIA GPU Feature Discovery, if present</li> <li>Label <code>node.kubernetes.io/instance-type</code>, if present</li> </ul> </li> <li>configuration of <code>nos</code> components<ul> <li>values provided during the Helm chart installation</li> </ul> </li> </ul> <p>Please find below an example of telemetry collection:</p> <pre><code>{\n\"installationUUID\": \"feb0a960-ed22-4882-96cf-ef0b83deaeb1\",\n\"nodes\": [\n{\n\"name\": \"node-1\",\n\"Capacity\": {\n\"cpu\": \"5\",\n\"memory\": \"7111996Ki\"\n},\n\"Labels\": {\n\"nvidia.com/gpu\": \"true\"\n},\n\"NodeInfo\": {\n\"kernelVersion\": \"5.15.49-linuxkit\",\n\"osImage\": \"Ubuntu 22.04.1 LTS\",\n\"containerRuntimeVersion\": \"containerd://1.6.7\",\n\"kubeletVersion\": \"v1.24.4\",\n\"architecture\": \"arm64\"\n}\n},\n{\n\"name\": \"node-2\",\n\"Capacity\": {\n\"cpu\": \"2\",\n\"memory\": \"7111996Ki\"\n},\n\"Labels\": null,\n\"NodeInfo\": {\n\"kernelVersion\": \"5.15.49-linuxkit\",\n\"osImage\": \"Ubuntu 22.04.1 LTS\",\n\"containerRuntimeVersion\": \"containerd://1.6.7\",\n\"kubeletVersion\": \"v1.24.4\",\n\"architecture\": \"arm64\"\n}\n\"chartValues\": {\n\"allowDefaultNamespace\": false,\n\"global\": {\n\"nvidiaGpuResourceMemoryGB\": 32\n}\n},\n\"components\": {\n\"nos-gpu-partitioner\": true,\n\"nos-scheduler\": true,\n\"nos-operator\": true\n}\n}\n]\n}\n</code></pre>"},{"location":"nos/telemetry/#how-to-opt-out","title":"How to opt-out?","text":"<p>You have two possibilities for opting-out:</p> <ol> <li>Set the value <code>shareTelemetry</code> to false when installing <code>nos</code> with the Helm Chart    <pre><code> helm install oci://ghcr.io/nebuly-ai/helm-charts/nos \\\n--version 0.1.2 \\\n--namespace nebuly-nos \\\n--generate-name \\\n--create-namespace \\\n--set shareTelemetry=false\n</code></pre></li> <li>Install <code>nos</code> without using Helm</li> </ol>"},{"location":"nos/telemetry/#should-i-opt-out","title":"Should I opt out?","text":"<p>Being open-source, we have very limited visibility into the use of the tool unless someone actively contacts us or opens an issue on GitHub.</p> <p>We would appreciate it if you would maintain telemetry, as it helps us improve the source code. In fact, it brings increasing value to the project and helps us to better prioritize feature development.</p> <p>We understand that you may still prefer not to share telemetry data and we respect that desire. Please follow the steps above to disable data collection.</p>"},{"location":"nos/developer/contribution-guidelines/","title":"Contribution guidelines","text":""},{"location":"nos/developer/contribution-guidelines/#how-to-submit-an-issue","title":"How to submit an issue","text":"<p>Did you spot a bug? Did you come up with a cool idea that you think should be implemented? Well, GitHub issues are the best way to let us know!</p> <p>We don't have a strict policy on issue generation: just use a meaningful title and specify the problem or your proposal in the first problem comment. Then, you can use GitHub labels to let us know what kind of proposal you are making, for example bug if you are reporting a new bug or enhancement if you are proposing a library improvement</p>"},{"location":"nos/developer/contribution-guidelines/#how-to-contribute-to-an-issue","title":"How to contribute to an issue","text":"<p>We are always delighted to welcome other people to the contributor section! We are looking forward to welcoming you to the community, but before you rush off and write 1000 lines of code, please take a few minutes to read our tips for contributing to the library.</p> <p>If it's one of your first contributions, check the tag good first issue \ud83c\udfc1</p> <ul> <li>Please fork the library instead of pulling it and creating a new branch.</li> <li>Work on your fork and work on your branch. Do not hesitate to ask questions by commenting on the issue or asking in the community chats.</li> <li>Open a pull request when you think the problem has been solved.</li> <li>In the pull request specify which problems it is solving/closing. For instance, if the pull request solves problem #1, the comment should be <code>Closes #1</code>.</li> <li>The title of the pull request must be meaningful and self-explanatory.</li> </ul>"},{"location":"nos/developer/contribution-guidelines/#coding-style","title":"Coding style","text":"<p>We use golangci-lint to enforce a consistent coding style. You can run the linter by using the following target: <pre><code>make lint\n</code></pre></p>"},{"location":"nos/developer/contribution-guidelines/#license","title":"License","text":"<p>All the source code files requires a license header. You can add automatically add it to new files by running: <pre><code>make license-fix\n</code></pre></p>"},{"location":"nos/developer/getting-started/","title":"Developer","text":""},{"location":"nos/developer/getting-started/#local-development","title":"Local development","text":"<p>We use Makefile targets for making it easy to setup a local development environment. You can list all the available targets by running <code>make help</code>.</p>"},{"location":"nos/developer/getting-started/#create-a-local-environment","title":"Create a local environment","text":"<p>You can create a local development environment just by running:</p> <pre><code>make cluster\n</code></pre> <p>The target uses Kind to create a local Kubernetes cluster that uses Docker containers as nodes.</p> <p>The nos operator uses webhooks that require SSL certificates. You can let cert-manager create and manage them by installing it on the cluster you have created in the previous step: <pre><code>make install-cert-manager\n</code></pre></p>"},{"location":"nos/developer/getting-started/#build-components","title":"Build components","text":"<p>You can build the nos components by running the <code>docker-build-&lt;component-name&gt;</code> targets. The targets build the Docker images using the default image name tagged with the version defined in the first line of the Makefile.</p> <p>Optionally, you can override the name and the tag of the Docker image by providing them as argument to the target.</p>"},{"location":"nos/developer/getting-started/#build-gpu-partitioner","title":"Build GPU Partitioner","text":"<p><pre><code>make docker-build-gpu-partitioner\n</code></pre> <pre><code>make docker-build-gpu-partitioner GPU_PARTITIONER_IMG=custom-image:tag\n</code></pre></p>"},{"location":"nos/developer/getting-started/#build-scheduler","title":"Build Scheduler","text":"<p><pre><code>make docker-build-scheduler\n</code></pre> <pre><code>make docker-build-scheduler SCHEDULER_IMG=custom-image:tag\n</code></pre></p>"},{"location":"nos/developer/getting-started/#build-operator","title":"Build Operator","text":"<p><pre><code>make docker-build-operator\n</code></pre> <pre><code>make docker-build-operator OPERATOR_IMG=custom-image:tag\n</code></pre></p>"},{"location":"nos/developer/getting-started/#build-mig-agent","title":"Build MIG Agent","text":"<p><pre><code>make docker-build-mig-agent\n</code></pre> <pre><code>make docker-build-mig-agent MIG_AGENT_IMG=custom-image:tag\n</code></pre></p>"},{"location":"nos/developer/getting-started/#build-gpu-agent","title":"Build GPU Agent","text":"<p><pre><code>make docker-build-gpu-agent\n</code></pre> <pre><code>make docker-build-gpu-agent GPU_AGENT_IMG=custom-image:tag\n</code></pre></p>"},{"location":"nos/developer/getting-started/#load-docker-images-into-the-cluster","title":"Load Docker images into the cluster","text":"<p>\u26a0\ufe0f If you use the tag <code>latest</code> Kubernetes will always download the image from the registry, ignoring the image you loaded into the cluster.</p> <p>You can load the Docker images you have built in the previous step into the cluster by running: <pre><code>kind load docker-image &lt;image-name&gt;:&lt;image-tag&gt;\n</code></pre></p>"},{"location":"nos/developer/getting-started/#install-components","title":"Install components","text":"<p>You can install single nos components by running: <pre><code>make deploy-&lt;component&gt;\n</code></pre> where <code>&lt;component&gt;</code> is one of the following: - <code>operator</code> - <code>gpu-partitioner</code> - <code>scheduler</code> - <code>mig-agent</code> - <code>gpu-agent</code></p> <p>The targets above installs the Docker images tagged with the version defined in the first line of the Makefile.</p> <p>You can override the Docker image name and tag by providing it as an argument to the target: <pre><code>make deploy-&lt;component&gt; &lt;COMPONENT&gt;_IMG=&lt;your-image&gt;\n</code></pre></p>"},{"location":"nos/dynamic-gpu-partitioning/configuration/","title":"Configuration","text":"<p>You can customize the GPU Partitioner settings by editing the values file of the nos Helm chart. In this section we focus on some of the values that you would typically want to customize.</p>"},{"location":"nos/dynamic-gpu-partitioning/configuration/#pods-batch-size","title":"Pods batch size","text":"<p>The GPU partitioner processes pending pods in batches of configurable size. You can set the batch size by editing the following two parameters of the configuration:</p> <ul> <li><code>batchWindowTimeoutSeconds</code>: timeout of the time window used for batching pending Pods. The time window starts when the GPU Partitioner starts processing a batch of pending Pods, and ends when the timeout expires or the batch is completed.</li> <li><code>batchWindowIdleSeconds</code>: idle time before a batch of pods is considered completed. Once the time window of a batch starts, if idle time elapses and no new pending pods are detected during this time, the batch is considered completed.</li> </ul> <p>Increase the value of these two parameters if you want the GPU partitioner to take into account more pending Pods when deciding the GPU partitioning plan, thus making potentially it more effective.</p> <p>Set lower values if you want the partitioning to be performed more frequently (e.g. if you want to react faster to changes in the cluster), and you don't mind if the partitioning is less effective (e.g. the resources requested by some pending pods might not be created).</p>"},{"location":"nos/dynamic-gpu-partitioning/configuration/#scheduler-configuration","title":"Scheduler configuration","text":"<p>The GPU Partitioner uses an internal scheduler to simulate the scheduling of the pending pods to determine whether a candidate GPU partitioning plan would make the pending pods schedulable.</p> <p>The GPU Partitioner reads the scheduler configuration from the ConfigMap defined by the field <code>gpuPartitioner.scheduler.config</code>, and it falls back to the default configuration if the ConfigMap is not found. You can edit this field to provide your custom scheduler configuration.</p> <p>If you installed <code>nos</code> with the <code>scheduler</code> flag enabled, the GPU Partitioner will use its configuration unless you specify a custom ConfigMap.</p>"},{"location":"nos/dynamic-gpu-partitioning/configuration/#available-mig-geometries","title":"Available MIG geometries","text":"<p>The GPU Partitioner determines the most proper partitioning plan to apply by considering the possible MIG geometries allowed each of the GPU models present in the cluster.</p> <p>You can set the MIG geometries supported by each GPU model by editing the <code>gpuPartitioner.knownMigGeometries</code> value of the installation chart.</p> <p>You can edit this file to add new MIG geometries for new GPU models, or to edit the existing ones according to your specific needs. For instance, you can remove some MIG geometries if you don't want to allow them to be used for a certain GPU model.</p>"},{"location":"nos/dynamic-gpu-partitioning/configuration/#how-it-works","title":"How it works","text":"<p>The GPU Partitioner component watches for pending pods that cannot be scheduled due to lack of MIG/MPS resources they request. If it finds such pods, it checks the current partitioning state of the GPUs in the cluster and tries to find a new partitioning state that would allow to schedule them without deleting any of the used resources.</p> <p>It does that by using an internal k8s scheduler, so that before choosing a candidate partitioning, the GPU Partitioner simulates the scheduling to check whether the partitioning would actually allow to schedule the pending Pods. If multiple partitioning configuration can be used to schedule the pending Pods, the one that would result in the highest number of schedulable pods is chosen.</p> <p>Moreover, just in the case of MIG partitioning, each specific GPU model allows to create only certain combinations of MIG profiles, which are called MIG geometries, so the GPU partitioner takes this constraint into account when trying to find a new partitioning. The available MIG geometries of each GPU model are defined in the field <code>gpuPartitioner.knownMigGeometries</code> field of the Helm chart.</p>"},{"location":"nos/dynamic-gpu-partitioning/configuration/#mig-partitioning","title":"MIG Partitioning","text":"<p>The actual partitioning specified by the GPU Partitioner for MIG GPUs is performed by the MIG Agent, which is a daemonset running on every node labeled with <code>nos.nebuly.com/gpu-partitioning: mig</code> that creates/deletes MIG profiles as requested by the GPU Partitioner.</p> <p>The MIG Agent exposes to the GPU Partitioner the used/free MIG resources of all the GPUs of the node on which it is running through the following node annotations:</p> <ul> <li><code>nos.nebuly.com/status-gpu-&lt;index&gt;-&lt;mig-profile&gt;-free: &lt;quantity&gt;</code></li> <li><code>nos.nebuly.com/status-gpu-&lt;index&gt;-&lt;mig-profile&gt;-used: &lt;quantity&gt;</code></li> </ul> <p>The MIG Agent also watches the node's annotations and, every time there desired MIG partitioning specified by the GPU Partitioner does not match the current state, it tries to apply it by creating and deleting the MIG profiles on the target GPUs. The GPU Partitioner specifies the desired MIG geometry of the GPUs of a node through annotations in the following format:</p> <p><code>nos.nebuly.com/spec-gpu-&lt;index&gt;-&lt;mig-profile&gt;: &lt;quantity&gt;</code></p> <p>Note that in some cases the MIG Agent might not be able to apply the desired MIG geometry specified by the GPU Partitioner. This can happen for two reasons:</p> <ol> <li>the MIG Agent never deletes MIG resources being in use by a Pod</li> <li>some MIG geometries require the MIG profiles to be created in a certain order, and due to reason (1) the MIG Agent might not be able to delete and re-create the existing MIG profiles in the order required by the new MIG geometry.</li> </ol> <p>In these cases, the MIG Agent tries to apply the desired partitioning by creating as many required resources as possible, in order to maximize the number of schedulable Pods. This can result in the MIG Agent applying the desired MIG geometry only partially.</p> <p>For further information regarding NVIDIA MIG and its integration with Kubernetes, please refer to the NVIDIA MIG User Guide and to the MIG Support in Kubernetes official documentation provided by NVIDIA.</p>"},{"location":"nos/dynamic-gpu-partitioning/configuration/#mps-partitioning","title":"MPS Partitioning","text":"<p>The creation and deletion of MPS resources is handled by the k8s-device-plugin, which can expose a single GPU as multiple MPS resources according to its configuration.</p> <p>When allocating a container requesting an MPS resource, the device plugin takes care of injecting theenvironment variables and mounting the volumes required by the container to communicate to the MPS server, making sure that the resource limits defined by the device requested by the container are enforced.</p> <p>For more information about MPS integration with Kubernetes you can refer to the Nebuly k8s-device-plugin documentation.</p>"},{"location":"nos/dynamic-gpu-partitioning/getting-started-mig/","title":"Getting started with MIG partitioning","text":"<p>Warning</p> <p>Multi-instance GPU (MIG) mode is supported only by NVIDIA GPUs based on Ampere, Hopper and newer architectures.</p>"},{"location":"nos/dynamic-gpu-partitioning/getting-started-mig/#prerequisites","title":"Prerequisites","text":"<p>To enable Dynamic MIG Partitioning on a certain node, the following prerequisites must be met:</p> <ul> <li>if a node has multiple GPUs, all the GPUs must be of the same model</li> <li>all the GPUs of the nodes for which you want to enable MIG partitioning must have MIG mode enabled</li> </ul>"},{"location":"nos/dynamic-gpu-partitioning/getting-started-mig/#enable-mig-mode","title":"Enable MIG mode","text":"<p>By default, MIG is not enabled on GPUs. In order to enable it, SSH into the node and run the following command for each GPU you want to enable MIG, where <code>&lt;index&gt;</code> corresponds to the index of each GPU:</p> <pre><code>sudo nvidia-smi -i &lt;index&gt; -mig 1\n</code></pre> <p>Depending on the kind of machine you are using, it may be necessary to reboot the node after enabling MIG mode for one of its GPUs.</p> <p>You can check whether MIG mode has been successfully enabled by running the following command and checking if you get a similar output:</p> <pre><code>$ nvidia-smi -i &lt;index&gt; --query-gpu=pci.bus_id,mig.mode.current --format=csv\n\npci.bus_id, mig.mode.current\n00000000:36:00.0, Enabled\n</code></pre> <p>For more information and troubleshooting you can refer to th NVIDIA documentation.</p>"},{"location":"nos/dynamic-gpu-partitioning/getting-started-mig/#enable-automatic-partitioning","title":"Enable automatic partitioning","text":"<p>You can enable automatic MIG partitioning on a node by adding to it the following label:</p> <pre><code>kubectl label nodes &lt;node-name&gt; \"nos.nebuly.com/gpu-partitioning=mig\"\n</code></pre> <p>The label delegates to <code>nos</code> the management of the MIG resources of all the GPUs of that node, so you don't have to manually configure the MIG geometry of the GPUs anymore: <code>nos</code> will dynamically create and delete the MIG profiles according to the resources requested by the pods submitted to the cluster, within the limits of the possible MIG geometries supported by each GPU model.</p> <p>The available MIG geometries supported by each GPU model are defined in a ConfigMap, which by default contains with the supported geometries of the most popular GPU models. You can override or extend the values of this ConfigMap by editing the field <code>gpuPartitioner.knownMigGeometries</code> of the installation chart.</p>"},{"location":"nos/dynamic-gpu-partitioning/getting-started-mig/#create-pods-requesting-mig-resources","title":"Create pods requesting MIG resources","text":"<p>Tip</p> <p>There is no need to manually create and manage MIG configurations. You can simply submit your Pods to the cluster and the requested MIG devices are automatically provisioned.</p> <p>You can make your pods request slices of GPU by specifying MIG devices in their containers requests:</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\nname: mig-partitioning-example\nspec:\ncontainers:\n- name: sleepy\nimage: \"busybox:latest\"\ncommand: [\"sleep\", \"120\"]\nresources:\nlimits:\nnvidia.com/mig-1g.10gb: 1\nEOF\n</code></pre> <p>In the example above, the pod requests a slice of a 10GB of memory, which is the smallest unit available in <code>NVIDIA-A100-80GB-PCIe</code> GPUs. If in your cluster you have different GPU models, the <code>nos</code> might not be able to create the specified MIG resource. You can find the MIG profiles supported by each GPU model in the NVIDIA documentation.</p> <p>Note</p> <p>Each container is supposed to request at most one MIG device. If a container needs more resources, then it should ask for a larger, single device as opposed to multiple smaller devices.</p>"},{"location":"nos/dynamic-gpu-partitioning/getting-started-mps/","title":"Getting started with MPS partitioning","text":"<p>Warning</p> <p>Multi-Process Service (MPS) is supported only by NVIDIA GPUs based on Volta and newer architectures.</p>"},{"location":"nos/dynamic-gpu-partitioning/getting-started-mps/#prerequisites","title":"Prerequisites","text":"<ul> <li>you need the Nebuly k8s-device-plugin installed on your cluster</li> </ul>"},{"location":"nos/dynamic-gpu-partitioning/getting-started-mps/#enable-automatic-partitioning","title":"Enable automatic partitioning","text":"<p>You can enable automatic MPS partitioning on a node by adding to it the following label:</p> <pre><code>kubectl label nodes &lt;node-name&gt; \"nos.nebuly.com/gpu-partitioning=mps\"\n</code></pre> <p>The label delegates to <code>nos</code> the management of the MPS resources of all the GPUs of that node. You just have to create submit your Pods to the cluster and  the requested MPS resources are automatically provisioned.</p>"},{"location":"nos/dynamic-gpu-partitioning/getting-started-mps/#create-pods-requesting-mps-resources","title":"Create pods requesting MPS resources","text":"<p>You can make your pods request slices of GPU by specifying MPS resources in their containers requests. MPS devices are exposed by our k8s-device-plugin using the following naming convention: <code>nvidia.com/gpu-&lt;size&gt;gb</code>, where <code>&lt;size&gt;</code> corresponds to the GB of memory of the GPU slice. The computing resources are instead equally shared among all its MPS resources.</p> <p>You can specify any size you want, but you should keep in mind that the GPU Partitioner will create an MPS resource on a certain GPU only if its size is smaller or equal than the total amount of memory of that GPU (which is indicated by the node label <code>nvidia.com/gpu.memory</code> applied by the NVIDIA GPU Operator).</p> <p>For instance, you can create a pod requesting a slice of a 10GB of GPU memory as follows:</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\nname: mps-partitioning-example\nspec:\nhostIPC: true # (2)\nsecurityContext:\nrunAsUser: 1000 # (3)\ncontainers:\n- name: sleepy\nimage: \"busybox:latest\"\ncommand: [\"sleep\", \"120\"]\nresources:\nlimits:\nnvidia.com/gpu-10gb: 1 # (1)\nEOF\n</code></pre> <ol> <li>Fraction of GPU with 10 GB of memory</li> <li><code>hostIPC</code> must be set to true</li> <li>Containers must run as the same user as the MPS Server</li> </ol> <p>Pods requesting MPS resources must meet two requirements:</p> <ol> <li><code>hostIPC</code> must be set to <code>true</code> in order to allow containers to access the IPC namespace of the host</li> <li>Containers must run as the same user as the user running the MPS server on the host, which is <code>1000</code> by default</li> </ol> <p>The two requirements above are due to how MPS works. Since it requires the clients and the server to share the same memory space, we need to allow the pods to access the host IPC namespace so that it can communicate with the MPS server running on it. Moreover, the MPS server accepts only connections from clients running as the same user as the server, which is <code>1000</code> by default (you can change it by setting the <code>mps.userID</code> value when installing the k8s-device-plugin chart), so the containers of your pods must run with the same user if they request MPS resources.</p> <p>Note</p> <p>Containers are supposed to request at most one MPS device. If a container needs more resources, then it should ask for a larger, single device as opposed to multiple smaller devices</p> <p>Warning</p> <p>If you run <code>nvidia-smi</code> inside a container, the output still shows the whole memory of the GPU. Nevertheless, processes inside the container are able to allocate only the amount of memory requested by the contaner. You can check the availble GPU memory through the environment variable <code>CUDA_MPS_PINNED_DEVICE_MEM_LIMIT</code>.</p>"},{"location":"nos/dynamic-gpu-partitioning/overview/","title":"Overview","text":"<p><code>nos</code> allows you to schedule Pods requesting fractions of GPUs. The GPUs are automatically partitioned into slices that can be requested by individual containers. In this way, GPUs are shared among multiple Pods increasing the overall utilization.</p> <p>The GPUs partitioning is performed automatically in real-time based on the requests of the Pods in your cluster. <code>nos</code> constantly watches the pending Pods and finds the best possible GPU partitioning configuration to schedule the highest number of the ones requesting fractions of GPUs.</p> <p>You can think of <code>nos</code> as a Cluster Autoscaler for GPUs: instead of adjusting the number of nodes and GPUs, it dynamically partitions them to maximize their utilization, leading to spare GPU capacity. Then, you can schedule more Pods or reduce the number of GPU nodes needed, reducing infrastructure costs.</p> <p>The GPU partitioning is performed either using Multi-instance GPU (MIG) or Multi-Process Service (MPS), depending on the partitioning mode you choose for each node.</p>"},{"location":"nos/dynamic-gpu-partitioning/partitioning-modes-comparison/","title":"Partitioning modes comparison","text":"<p>The following tables summarizes the difference between the different partitioning modes supported by NVIDIA GPUs. Note that they are not mutually exclusive: <code>nos</code> allows you to choose a different partitioning mode for each node in your cluster according to your needs and available hardware.</p> Partitioning mode Supported by <code>nos</code> Workload isolation level Pros Cons Multi-instance GPU (MIG) \u2705 Best <ul><li>Processes are executed in parallel</li><li>Full isolation (dedicated memory and compute resources)</li></ul> <ul><li>Supported by fewer GPU models (only Ampere or more recent architectures)</li><li>Coarse-grained control over memory and compute resources</li></ul> Multi-process server (MPS) \u2705 Medium <ul><li>Processes are executed parallel</li><li>Fine-grained control over memory and compute resources allocation</li></ul> <ul><li>No error isolation and memory protection</li></ul> Time-slicing \u274c None <ul><li>Processes are executed concurrently</li><li>Supported by older GPU architectures (Pascal or newer)</li></ul> <ul><li>No resource limits</li><li>No memory isolation</li><li>Lower performance due to context-switching overhead</li></ul>"},{"location":"nos/dynamic-gpu-partitioning/partitioning-modes-comparison/#multi-instance-gpu-mig","title":"Multi-instance GPU (MIG)","text":"<p>Multi-instance GPU (MIG) is a technology available on NVIDIA Ampere or more recent architectures that allows to securely partition a GPU into separate GPU instances for CUDA applications, each fully isolated with its own high-bandwidth memory, cache, and compute cores.</p> <p>The isolated GPU slices are called MIG devices, and they are named adopting a format that indicates the compute and memory resources of the device. For example, 2g.20gb corresponds to a GPU slice with 20 GB of memory.</p> <p>MIG does not allow to create GPU slices of custom sizes and quantity, as each GPU model only supports a specific set of MIG profiles. This reduces the degree of granularity with which you can partition the GPUs. Additionally, the MIG devices must be created respecting certain placement rules, which further limits flexibility of use.</p> <p>MIG is the GPU sharing approach that offers the highest level of isolation among processes. However, it lacks in flexibility and it is compatible only with few GPU architectures (Ampere and Hopper).</p> <p>You can find out more on how MIG technology works in the official NVIDIA MIG User Guide.</p>"},{"location":"nos/dynamic-gpu-partitioning/partitioning-modes-comparison/#multi-process-service-mps","title":"Multi-Process Service (MPS)","text":"<p>Multi-Process Service (MPS) is a client-server implementation of the CUDA Application Programming Interface (API) for running multiple processes concurrently on the same GPU:</p> <ul> <li>the server manages GPU access providing concurrency between clients</li> <li>clients connect to the server through the client runtime, which is built into the CUDA Driver library and may be used transparently by any CUDA application.</li> </ul> <p>The main advantage of MPS is that it provides a fine-grained control over the GPU assigned to each client, allowing to specify arbitrary limits on both the amount of allocatable memory and the available compute. The Nebuly k8s-device-plugin takes advantage of this feature for exposing to Kubernetes GPU resources with an arbitrary amount of allocatable memory defined by the user.</p> <p>Compared to time-slicing, MPS eliminates the overhead of context-switching by running processes in parallel through spatial sharing, and therefore leads to better compute performance. Moreover, MPS provides each client with its own GPU memory address space. This allows to enforce memory limits on the processes overcoming the limitations of time-slicing sharing.</p> <p>It is however important to point out that processes sharing a GPU through MPS are not fully isolated from each other. Indeed, even though MPS allows to limit clients' compute and memory resources, it does not provide error isolation and memory protection. This means that a client process can crash and cause the entire GPU to reset, impacting all other processes running on the GPU. However, this issue can often be addressed by properly handling CUDA errors and SIGTERM signals.</p>"},{"location":"nos/dynamic-gpu-partitioning/partitioning-modes-comparison/#time-slicing","title":"Time-slicing","text":"<p>Time-slicing consists of oversubscribing a GPU leveraging its time-slicing scheduler, which executes multiple CUDA processes concurrently through temporal sharing.</p> <p>This means that the GPU shares its compute resources among the different processes in a fair-sharing manner by switching between processes at regular intervals of time. This generates a computing time overhead related to the continuous context switching, which translates into jitter and higher latency.</p> <p>Time-slicing is supported by basically every GPU architecture and is the simplest solution for sharing a GPU in a Kubernetes cluster. However, constant switching among processes creates a computation time overhead. Also, time-slicing does not provide any level of memory isolation among the processes sharing a GPU, nor any memory allocation limits, which can lead to frequent Out-Of-Memory (OOM) errors.</p> <p>Info</p> <p>Given the drawbacks above the availability of more robust technologies such as MIG and MPS, at the moment we decided to not support time-slicing GPU sharing in <code>nos</code>.</p>"},{"location":"nos/dynamic-gpu-partitioning/troubleshooting/","title":"Troubleshooting","text":"<p>If you run into issues with Automatic GPU Partitioning, you can troubleshoot by checking the logs of the GPU Partitioner and MIG Agent pods. You can do that by running the following commands:</p> <p>Check GPU Partitioner logs:</p> <pre><code> kubectl logs -n nebuly-nos -l app.kubernetes.io/component=nos-gpu-partitioner -f\n</code></pre> <p>Check MIG Agent logs:</p> <pre><code> kubectl logs -n nebuly-nos -l app.kubernetes.io/component=nos-mig-agent -f\n</code></pre> <p>Check Nebuly's device-plugin logs:</p> <pre><code>kubectl logs -n nebuly-nvidia -l app.kubernetes.io/name=nebuly-nvidia-device-plugin -f\n</code></pre>"},{"location":"nos/elastic-resource-quota/configuration/","title":"Configuration","text":""},{"location":"nos/elastic-resource-quota/configuration/#scheduler-installation-options","title":"Scheduler installation options","text":"<p>You can add scheduling support for Elastic Resource Quota to your cluster by choosing one of the following options. In both cases, you also need to install the <code>nos operator</code> to manage the CRDs.</p>"},{"location":"nos/elastic-resource-quota/configuration/#option-1-use-nos-scheduler-recommended","title":"Option 1 - Use nos scheduler (recommended)","text":"<p>This is the recommended option. You can deploy the nos scheduler to your cluster either as the default scheduler or as a second scheduler that runs alongside the default one. In the latter case, you can use the <code>schedulerName</code> field of the Pod spec to specify which scheduler should be used.</p> <p>If you installed <code>nos</code> through the Helm chart, the scheduler is deployed automatically unless you set the value <code>scheduler.enabled=false</code>.</p>"},{"location":"nos/elastic-resource-quota/configuration/#option-2-use-your-k8s-scheduler","title":"Option 2 - Use your k8s scheduler","text":"<p>Since nos Elastic Quota support is implemented as a scheduler plugin, you can compile it into your k8s scheduler and then enable it through the kube-scheduler configuration as follows:</p> <pre><code>apiVersion: kubescheduler.config.k8s.io/v1beta2\nkind: KubeSchedulerConfiguration\nleaderElection:\nleaderElect: false\nprofiles:\n- schedulerName: default-scheduler\nplugins:\npreFilter:\nenabled:\n- name: CapacityScheduling\npostFilter:\nenabled:\n- name: CapacityScheduling\ndisabled:\n- name: \"*\"\nreserve:\nenabled:\n- name: CapacityScheduling\npluginConfig:\n- name: CapacityScheduling\nargs:\n# Defines how much GB of memory does a nvidia.com/gpu has.\nnvidiaGpuResourceMemoryGB: 32\n</code></pre> <p>In order to compile the plugin with your scheduler, you just need to add the following line to the <code>main.go</code> file of your scheduler:</p> <pre><code>package main\nimport (\n\"github.com/nebuly-ai/nos/pkg/scheduler/plugins/capacityscheduling\"\n\"k8s.io/kubernetes/cmd/kube-scheduler/app\"\n// Import plugin config\n\"github.com/nebuly-ai/nos/pkg/api/scheduler\"\n\"github.com/nebuly-ai/nos/pkg/api/scheduler/v1beta3\"\n// Ensure nos.nebuly.com/v1alpha1 package is initialized\n_ \"github.com/nebuly-ai/nos/pkg/api/nos.nebuly.com/v1alpha1\"\n)\nfunc main() {\n// - rest of your code here -\n// Add plugin config to scheme\nutilruntime.Must(scheduler.AddToScheme(scheme))\nutilruntime.Must(v1beta3.AddToScheme(scheme))\n// Add plugin to scheduler command\ncommand := app.NewSchedulerCommand(\n// - your other plugins here -\napp.WithPlugin(capacityscheduling.Name, capacityscheduling.New),\n)\n// - rest of your code here -\n}\n</code></pre> <p>If you choose this installation option, you don't need to deploy <code>nos</code> scheduler, so you can disable it by setting <code>--set scheduler.enabled=false</code> when installing the <code>nos</code> chart.</p>"},{"location":"nos/elastic-resource-quota/getting-started/","title":"Getting started","text":""},{"location":"nos/elastic-resource-quota/getting-started/#create-elastic-quotas","title":"Create elastic quotas","text":"<pre><code>$ kubectl apply -f -- &lt;&lt;EOF\napiVersion: nos.nebuly.com/v1alpha1\nkind: ElasticQuota\nmetadata:\nname: quota-a\nnamespace: team-a\nspec:\nmin:\ncpu: 2\nnos.nebuly.com/gpu-memory: 16\nmax:\ncpu: 10\nEOF\n</code></pre> <p>The example above creates a quota for the namespace <code>team-a</code>, guaranteeing it 2 CPUs and 16 GB of GPU memory, and limiting the maximum number of CPUs it can use to 10. Note that:</p> <ul> <li>the <code>max</code> field is optional. If it is not specified, then the Elastic Quota does not enforce any upper limits on the amount resources that can be created in the namespace</li> <li>you can specify any valid Kubernetes resource you want in <code>max</code> and <code>min</code> fields</li> </ul>"},{"location":"nos/elastic-resource-quota/getting-started/#create-pods-subject-to-elastic-resource-quota","title":"Create Pods subject to Elastic Resource Quota","text":"<p>Unless you deployed the <code>nos</code> scheduler as the default scheduler for your cluster, you need to instruct Kubernetes to use it for scheduling the Pods you want to be subject to Elastic Resource Quotas.</p> <p>You can do that by setting the value of the <code>schedulerName</code> field of your Pods specification to <code>scheduler</code> (or to any name you chose when installing <code>nos</code>), as shown in the example below.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: my-pod\nspec:\nschedulerName: nos-scheduler\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\n</code></pre>"},{"location":"nos/elastic-resource-quota/getting-started/#how-to-define-resource-quotas","title":"How to define resource quotas","text":"<p>You can define resource limits on namespaces using two custom resources: <code>ElasticQuota</code> and <code>CompositeElasticQuota</code>. They both work in the same way, the only difference is that the latter defines limits on multiple namespaces instead of on a single one. Limits are specified through two fields:</p> <ul> <li><code>min</code>: the minimum resources that are guaranteed to the namespace. <code>nos</code> will make sure that, at any time, the namespace subject to the quota will always have access to at least these resources.</li> <li><code>max</code>: optional field that limits the total amount of resources that can be requested by a namespace. If not max is not specified, then <code>nos</code> does not enforce any upper limits on the resources that can be requested by the namespace.</li> </ul> <p>You can find sample definitions of these resources under the samples directory.</p> <p>Note that <code>ElasticQuota</code> and <code>CompositeElasticQuota</code> are treated by <code>nos</code> in the same way: a namespace subject to an <code>ElasticQuota</code> can borrow resources from namespaces subject to either other elastic quotas or composite elastic quotas and, vice-versa, namespaces subject to a <code>CompositeElasticQuota</code> can borrow resources from namespaces subject to either elastic quotas or composite elastic quotas.</p>"},{"location":"nos/elastic-resource-quota/getting-started/#constraints","title":"Constraints","text":"<p>The following constraints are enforced over elastic quota resources:</p> <ul> <li>you can create at most one <code>ElasticQuota</code> per namespace</li> <li>a namespace can be subject either to one <code>ElasticQuota</code> or one <code>CompositeElasticQuota</code>, but not both at the same time</li> <li>if a quota resource specifies both <code>max</code> and <code>min</code> fields, then the value of the resources specified in <code>max</code> must be greater or equal than the ones specified in <code>min</code></li> </ul>"},{"location":"nos/elastic-resource-quota/getting-started/#how-used-resources-are-computed","title":"How used resources are computed","text":"<p>When a namespace is subject to an ElasticQuota (or to a CompositeElasticQuota), <code>nos</code> computes the number of quotas consumed by that namespace by aggregating the resources requested by its pods, considering only the ones whose phase is <code>Running</code>. In this way, <code>nos</code> avoid lower resource utilization due to scheduled pods that failed to start.</p> <p>Every time the amount of resources consumed by a namespace changes (e.g a Pod changes its phase to or from <code>Running</code>), the status of the respective quota object gets updated with the new amount of used resources.</p> <p>You can check how many resources have been consumed by each namespace by looking at the field <code>used</code> of the <code>ElasticQuota</code> and <code>CompositeElasticQuota</code> objects status.</p>"},{"location":"nos/elastic-resource-quota/key-concepts/","title":"Key concepts","text":""},{"location":"nos/elastic-resource-quota/key-concepts/#over-quotas","title":"Over-quotas","text":"<p>If a namespace subject to an <code>ElasticQuota</code> (or, equivalently, to a <code>CompositeElasticQuota</code>) is using all the resources guaranteed by the <code>min</code> field of its quota, it can still host new pods by \"borrowing\" quotas from other namespaces which has available resources (e.g. from namespaces subject to other quotas where <code>min</code> resources are not being completely used).</p> <p>Info</p> <p>Pods that are scheduled \"borrowing\" unused quotas from other namespaces are called over-quota pods.</p> <p>Over-quota pods can be preempted at any time to free up resources if any of the namespaces lending the quotas claims back its resources.</p> <p>You can check whether a Pod is in over-quota by checking the value of the label <code>nos.nebuly.com/capacity</code>, which is automatically created and updated by the nos operator for every Pod created in a namespace subject to an ElasticQuota or to a CompositeElasticQuota. The two possible values for this label are <code>in-quota</code> and <code>over-quota</code>.</p> <p>You can use this label to easily find out at any time which are the over-quota pods subject to preemption risk:</p> <pre><code>kubectl get pods --all-namespaces -l nos.nebuly.com/capacity=\"over-quota\"\n</code></pre>"},{"location":"nos/elastic-resource-quota/key-concepts/#how-over-quota-pods-are-labelled","title":"How over-quota pods are labelled","text":"<p>All the pods created within a namespace subject to a quota are labelled as <code>in-quota</code> as long as the <code>used</code> resources of the quota do not exceed its <code>min</code> resources. When this happens and news pods are created in that namespace, they are labelled as <code>over-quota</code> when they reach the running phase.</p> <p><code>nos</code> re-evaluates the over-quota status of each Pod of a namespace every time a new Pod in that namespace changes its phase to/from \"Running\". With the default configuration, <code>nos</code> sorts the pods by creation date and, if the creation timestamp is the same, by requested resources, placing first the pods with older creation timestamp and with fewer requested resources. After the pods are sorted, <code>nos</code> computes the aggregated requested resources by summing the request of each Pod, and it marks as <code>over-quota</code> all the pods for which <code>used</code> is greater than <code>min</code>.</p> <p>\ud83d\udea7 Soon it will be possible to customize the order criteria used for sorting the pods during this process through the nos-operator configuration.</p>"},{"location":"nos/elastic-resource-quota/key-concepts/#over-quota-fair-sharing","title":"Over-quota fair sharing","text":"<p>In order to prevent a single namespace from consuming all the over-quotas available in the cluster and starving the others, <code>nos</code> implements a fair-sharing mechanism that guarantees that each namespace subject to an ElasticQuota has right to a part of the available over-quotas proportional to its <code>min</code> field.</p> <p>The fair-sharing mechanism does not enforce any hard limit on the amount of over-quotas pods that a namespace can have, but instead it implements fair sharing by preemption. Specifically, a Pod-A subject to elastic-quota-A can preempt Pod-b subject to elastic-quota-B if the following conditions are met:</p> <ol> <li>Pod-B is in over-quota</li> <li><code>used</code> field of Elastic-quota-A + Pod-A request &lt;= guaranteed over-quotas A</li> <li>used over-quotas of Elastic-quota-B &gt; guaranteed over-quotas B</li> </ol> <p>Where:</p> <ul> <li>guaranteed over-quotas A = percentage of guaranteed over-quotas A * tot. available over-quotas</li> <li>percentage of guaranteed over-quotas A = min A / sum(min_i) * 100</li> <li>tot. available over-quotas = sum( max(0, min_i - used_i ) )</li> </ul>"},{"location":"nos/elastic-resource-quota/key-concepts/#example","title":"Example","text":"<p>Let's assume we have a K8s cluster with the following Elastic Quota resources:</p> Elastic Quota Min Max Elastic Quota A nos.nebuly.com/gpu-memory: 40 None Elastic Quota B nos.nebuly.com/gpu-memory: 10 None Elastic Quota C nos.nebuly.com/gpu-memory: 30 None <p>The table below shows the quotas usage of the cluster at two different times:</p> Time Elastic Quota A Elastic Quota B Elastic Quota C t1 Used: 40/40 GB Used: 40/10 GB Over-quota: 30 GB Used: 0 GB t2 Used: 50/40 GB Used 30/10 GB Over-quota: 20 GB Used: 0 GB <p>The cluster has a total of 30 GB of memory of available over-quotas, which at time t1 are all being consumed by the pods in the namespace subject to Elastic Quota B.</p> <p>At time t2, a new Pod is created in the namespace subject to Elastic Quota A. Even though all the quotas of the cluster are currently being used, the fair sharing mechanism grants to Elastic Quota A a certain amount of over-quotas that it can use, and in order to grant these quotas nos can preempt one or more over-quota pods from the namespace subject to Elastic Quota B.</p> <p>Specifically, the following are the amounts of over-quotas guaranteed to each of the namespaces subject to the Elastic Quotas defined in the table above:</p> <ul> <li>guaranteed over-quota A = 40 / (40 + 10 + 30) * (0 + 0 + (30 - 0)) = 15</li> <li>guaranteed over-quota B = 10 / (40 + 10 + 30) * (0 + 0 + (30 - 0)) = 3</li> </ul> <p>Assuming that all the pods in the cluster are requesting only 10 GB of GPU memory, an over-quota Pod from Elastic Quota B is preempted because the following conditions are true:</p> <ul> <li>\u2705 used quotas A + new Pod A &lt;= min quota A + guaranteed over-quota A</li> <li>40 + 10 &lt;= 40 + 15</li> <li>\u2705 used over-quotas B &gt; guaranteed over-quotas</li> <li>30 &gt; 3</li> </ul>"},{"location":"nos/elastic-resource-quota/key-concepts/#gpu-memory-limits","title":"GPU memory limits","text":"<p>Both <code>ElasticQuota</code> and <code>CompositeElasticQuota</code> resources support the custom resource <code>nos.nebuly.com/gpu-memory</code>. You can use this resource in the <code>min</code> and <code>max</code> fields of the elastic quotas specification to define the minimum amount of GPU memory (expressed in GB) guaranteed to a certain namespace and its maximum limit, respectively.</p> <p>This resource is particularly useful if you use Elastic Quotas together with automatic GPU partitioning, since it allows you to assign resources to different teams (e.g. namespaces) in terms of GPU memory instead of in number of GPUs, and the users can than consume request in the same terms by claiming GPU slices with a specific amount of memory, enabling an overall fine-grained control over the GPUs of the cluster.</p> <p><code>nos</code> automatically computes the GPU memory requested by each Pod from the GPU resources requested by its containers and enforces the limits accordingly. The amount of memory GB corresponding to the generic resource <code>nvidia.com/gpu</code> is defined by the field <code>global.nvidiaGpuResourceMemoryGB</code> of the installation chart, which is <code>32</code> by default.</p> <p>For instance, using the default configuration, the value of the resource <code>nos.nebuly.com/gpu-memory</code> computed from the Pod specification below is <code>10+32=42</code>.</p> <pre><code>apiVersion: apps/v1\nkind: Pod\nmetadata:\nname: nginx-deployment\nspec:\nschedulerName: nos-scheduler\ncontainers:\n- name: my-container\nimage: my-image:0.0.1\nresources:\nlimits:\nnvidia.com/mig-1g.10gb: 1\nnvidia.com/gpu: 1\n</code></pre>"},{"location":"nos/elastic-resource-quota/overview/","title":"Overview","text":"<p><code>nos</code> extends the Kubernetes Resource Quotas by implementing the Capacity Scheduling KEP and adding more flexibility through two custom resources: <code>ElasticQuotas</code> and <code>CompositeElasticQuotas</code>.</p> <p>While standard Kubernetes resource quotas allow you only to define limits on the maximum overall resource allocation of each namespace, <code>nos</code> elastic quotas let you define two different limits:</p> <ol> <li><code>min</code>: the minimum resources that are guaranteed to the namespace</li> <li><code>max</code>: the upper bound of the resources that the namespace can consume</li> </ol> <p>In this way namespaces can borrow reserved resource quotas from other namespaces that are not using them, as long as they do not exceed their max limit (if any) and the namespaces lending the quotas do not need them. When a namespace claims back its reserved <code>min</code> resources, pods borrowing resources from other namespaces (e.g. over-quota pods) are preempted to make up space.</p> <p>Moreover, while the standard Kubernetes quota management computes the used quotas as the aggregation of the resources of the resource requests specified in the Pods spec, <code>nos</code> computes the used quotas by taking into account only running Pods in order to avoid lower resource utilization due to scheduled Pods that failed to start.</p> <p>Elastic Resource Quota management is based on the Capacity Scheduling scheduler plugin, which also implements the Capacity Scheduling KEP. <code>nos</code> extends the former implementation by adding the following features:</p> <ul> <li>over-quota pods preemption</li> <li><code>CompositeElasticQuota</code> resources for defining limits on multiple namespaces</li> <li>custom resource <code>nos.nebuly.com/gpu-memory</code></li> <li>fair sharing of over-quota resources</li> <li>optional <code>max</code> limits</li> </ul>"},{"location":"nos/elastic-resource-quota/troubleshooting/","title":"Troubleshooting","text":"<p>You can check the logs of the scheduler by running the following command:</p> <pre><code> kubectl logs -n nebuly-nos -l app.kubernetes.io/component=nos-scheduler -f\n</code></pre> <p>You can check the logs of the operator by running the following command:</p> <pre><code> kubectl logs -n nebuly-nos -l app.kubernetes.io/component=nos-operator -f\n</code></pre>"},{"location":"nos/helm-charts/nos/","title":"nos","text":"<p>The open-source platform for running AI workloads on k8s in an optimized way, both in terms of hardware utilization and workload performance.</p>"},{"location":"nos/helm-charts/nos/#maintainers","title":"Maintainers","text":"Name Email Url Michele Zanotti m.zanotti@nebuly.com Diego Fiori d.fiori@nebuly.com"},{"location":"nos/helm-charts/nos/#source-code","title":"Source Code","text":"<ul> <li>https://github.com/nebuly-ai/nos</li> </ul>"},{"location":"nos/helm-charts/nos/#values","title":"Values","text":"Key Type Default Description allowDefaultNamespace bool <code>false</code> If true allows to deploy <code>nos</code> chart in the <code>default</code> namespace gpuPartitioner.affinity object <code>{}</code> Sets the affinity config of the GPU Partitioner Pod. gpuPartitioner.batchWindowIdleSeconds int <code>10</code> Idle seconds before the GPU partitioner processes the current batch if no new pending Pods are created, and the timeout has not been reached.  Higher values make the GPU partitioner will potentially take into account more pending Pods when deciding the GPU partitioning plan, but the partitioning will be performed less frequently gpuPartitioner.batchWindowTimeoutSeconds int <code>60</code> Timeout of the window used by the GPU partitioner for batching pending Pods.  Higher values make the GPU partitioner will potentially take into account more pending Pods when deciding the GPU partitioning plan, but the partitioning will be performed less frequently gpuPartitioner.devicePlugin.config.name string <code>\"nos-device-plugin-configs\"</code> Name of the ConfigMap containing the NVIDIA Device Plugin configuration files. It must be equal to the value \"devicePlugin.config.name\" of the Helm chart used for deploying the NVIDIA GPU Operator. gpuPartitioner.devicePlugin.config.namespace string <code>\"nebuly-nvidia\"</code> Namespace of the ConfigMap containing the NVIDIA Device Plugin configuration files. It must be equal to the namespace where the Nebuly NVIDIA Device Plugin has been deployed to. gpuPartitioner.devicePlugin.configUpdateDelaySeconds int <code>5</code> Duration of the delay between when the new partitioning config is computed and when it is sent to the NVIDIA device plugin. Since the config is provided to the plugin as a mounted ConfigMap, this delay is required to ensure that the updated ConfigMap is propagated to the mounted volume. gpuPartitioner.enabled bool <code>true</code> Enable or disable the <code>nos gpu partitioner</code> gpuPartitioner.fullnameOverride string <code>\"\"</code> gpuPartitioner.gpuAgent object - Configuration of the GPU Agent component of the GPU Partitioner. gpuPartitioner.gpuAgent.image.pullPolicy string <code>\"IfNotPresent\"</code> Sets the GPU Agent Docker image pull policy. gpuPartitioner.gpuAgent.image.repository string <code>\"ghcr.io/nebuly-ai/nos-gpu-agent\"</code> Sets the GPU Agent Docker image. gpuPartitioner.gpuAgent.image.tag string <code>\"\"</code> Overrides the GPU Agent image tag whose default is the chart appVersion. gpuPartitioner.gpuAgent.logLevel int <code>0</code> The level of log of the GPU Agent. Zero corresponds to <code>info</code>, while values greater or equal than 1 corresponds to higher debug levels. Must be &gt;= 0. gpuPartitioner.gpuAgent.reportConfigIntervalSeconds int <code>10</code> Interval at which the mig-agent will report to k8s status of the GPUs of the Node gpuPartitioner.gpuAgent.resources object <code>{\"limits\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}</code> Sets the resource requests and limits of the GPU Agent container. gpuPartitioner.gpuAgent.tolerations list <code>[{\"effect\":\"NoSchedule\",\"key\":\"kubernetes.azure.com/scalesetpriority\",\"operator\":\"Equal\",\"value\":\"spot\"}]</code> Sets the tolerations of the GPU Agent Pod. gpuPartitioner.image.pullPolicy string <code>\"IfNotPresent\"</code> Sets the GPU Partitioner Docker image pull policy. gpuPartitioner.image.repository string <code>\"ghcr.io/nebuly-ai/nos-gpu-partitioner\"</code> Sets the GPU Partitioner Docker image. gpuPartitioner.image.tag string <code>\"\"</code> Overrides the GPU Partitioner image tag whose default is the chart appVersion. gpuPartitioner.knownMigGeometries list - List that associates GPU models to the respective allowed MIG configurations gpuPartitioner.kubeRbacProxy object - Configuration of the Kube RBAC Proxy, which runs as sidecar of all the GPU Partitioner components Pods. gpuPartitioner.leaderElection.enabled bool <code>true</code> Enables/Disables the leader election of the GPU Partitioner controller manager. gpuPartitioner.logLevel int <code>0</code> The level of log of the GPU Partitioner. Zero corresponds to <code>info</code>, while values greater or equal than 1 corresponds to higher debug levels. Must be &gt;= 0. gpuPartitioner.migAgent object - Configuration of the MIG Agent component of the GPU Partitioner. gpuPartitioner.migAgent.image.pullPolicy string <code>\"IfNotPresent\"</code> Sets the MIG Agent Docker image pull policy. gpuPartitioner.migAgent.image.repository string <code>\"ghcr.io/nebuly-ai/nos-mig-agent\"</code> Sets the MIG Agent Docker image. gpuPartitioner.migAgent.image.tag string <code>\"\"</code> Overrides the MIG Agent image tag whose default is the chart appVersion. gpuPartitioner.migAgent.logLevel int <code>0</code> The level of log of the MIG Agent. Zero corresponds to <code>info</code>, while values greater or equal than 1 corresponds to higher debug levels. Must be &gt;= 0. gpuPartitioner.migAgent.reportConfigIntervalSeconds int <code>10</code> Interval at which the mig-agent will report to k8s the MIG partitioning status of the GPUs of the Node gpuPartitioner.migAgent.resources object <code>{\"limits\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}</code> Sets the resource requests and limits of the MIG Agent container. gpuPartitioner.migAgent.tolerations list <code>[{\"effect\":\"NoSchedule\",\"key\":\"kubernetes.azure.com/scalesetpriority\",\"operator\":\"Equal\",\"value\":\"spot\"}]</code> Sets the tolerations of the MIG Agent Pod. gpuPartitioner.nameOverride string <code>\"\"</code> gpuPartitioner.nodeSelector object <code>{}</code> Sets the nodeSelector config of the GPU Partitioner Pod. gpuPartitioner.podAnnotations object <code>{}</code> Sets the annotations of the GPU Partitioner Pod. gpuPartitioner.podSecurityContext object <code>{\"runAsNonRoot\":true,\"runAsUser\":1000}</code> Sets the security context of the GPU partitioner Pod. gpuPartitioner.replicaCount int <code>1</code> Number of replicas of the gpu-manager Pod. gpuPartitioner.resources object <code>{\"limits\":{\"cpu\":\"500m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"10m\",\"memory\":\"64Mi\"}}</code> Sets the resource limits and requests of the GPU partitioner container. gpuPartitioner.scheduler.config.name string <code>\"nos-scheduler-config\"</code> Name of the ConfigMap containing the k8s scheduler configuration file. If not specified or the ConfigMap does not exist, the GPU partitioner will use the default k8s scheduler profile. gpuPartitioner.tolerations list <code>[]</code> Sets the tolerations of the GPU Partitioner Pod. nvidiaGpuResourceMemoryGB int <code>32</code> Defines how many GB of memory each nvidia.com/gpu resource has. operator.affinity object <code>{}</code> Sets the affinity config of the operator Pod. operator.enabled bool <code>true</code> Enable or disable the <code>nos operator</code> operator.fullnameOverride string <code>\"\"</code> operator.image.pullPolicy string <code>\"IfNotPresent\"</code> Sets the operator Docker image pull policy. operator.image.repository string <code>\"ghcr.io/nebuly-ai/nos-operator\"</code> Sets the operator Docker repository operator.image.tag string <code>\"\"</code> Overrides the operator Docker image tag whose default is the chart appVersion. operator.kubeRbacProxy object - Configuration of the Kube RBAC Proxy, which runs as sidecar of the operator Pods. operator.leaderElection.enabled bool <code>true</code> Enables/Disables the leader election of the operator controller manager. operator.logLevel int <code>0</code> The level of log of the controller manager. Zero corresponds to <code>info</code>, while values greater or equal than 1 corresponds to higher debug levels. Must be &gt;= 0. operator.nameOverride string <code>\"\"</code> operator.nodeSelector object <code>{}</code> Sets the nodeSelector config of the operator Pod. operator.podAnnotations object <code>{}</code> Sets the annotations of the operator Pod. operator.podSecurityContext object <code>{\"runAsNonRoot\":true}</code> Sets the security context of the operator Pod. operator.replicaCount int <code>1</code> Number of replicas of the controller manager Pod. operator.resources object <code>{\"limits\":{\"cpu\":\"500m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"10m\",\"memory\":\"64Mi\"}}</code> Sets the resource limits and requests of the operator controller manager container. operator.securityContext object <code>{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]}}</code> Sets the security context of the operator container. operator.tolerations list <code>[]</code> Sets the tolerations of the operator Pod. scheduler.affinity object <code>{}</code> Sets the affinity config of the scheduler deployment. scheduler.config object <code>{}</code> Overrides the Kube Scheduler configuration scheduler.enabled bool <code>true</code> Enable or disable the <code>nos scheduler</code> scheduler.fullnameOverride string <code>\"\"</code> scheduler.image.pullPolicy string <code>\"IfNotPresent\"</code> Sets Docker image pull policy. scheduler.image.repository string <code>\"ghcr.io/nebuly-ai/nos-scheduler\"</code> Sets Docker image. scheduler.image.tag string <code>\"\"</code> Overrides the image tag whose default is the chart appVersion. scheduler.leaderElection.enabled bool <code>true</code> Enables/Disables the leader election when deployed with multiple replicas. scheduler.logLevel int <code>0</code> The level of log of the scheduler. Zero corresponds to <code>info</code>, while values greater or equal than 1 corresponds to higher debug levels. Must be &gt;= 0. scheduler.nameOverride string <code>\"\"</code> scheduler.nodeSelector object <code>{}</code> Sets the nodeSelector config of the scheduler deployment. scheduler.podAnnotations object <code>{}</code> Sets the annotations of the scheduler Pod. scheduler.podSecurityContext object <code>{}</code> Sets the security context of the scheduler Pod scheduler.replicaCount int <code>1</code> Number of replicas of the scheduler. scheduler.resources object <code>{\"limits\":{\"cpu\":\"500m\",\"memory\":\"128Mi\"},\"requests\":{\"cpu\":\"10m\",\"memory\":\"64Mi\"}}</code> Sets the resource limits and requests of the scheduler container. scheduler.securityContext object <code>{\"privileged\":false}</code> Sets the security context of the scheduler container scheduler.tolerations list <code>[]</code> Sets the tolerations of the scheduler deployment. shareTelemetry bool <code>true</code> If true, shares with Nebuly telemetry data collected only during the Chart installation"}]}