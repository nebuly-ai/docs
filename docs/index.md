# Quickstart

`Nebullvm` is an ecosystem of plug and play modules to boost the performances of your AI systems. The optimization modules are stack-agnostic and work with any library. They are designed to be easily integrated into your system, providing a quick and seamless boost to its performance. Simply plug and play to start realizing the benefits of optimized performance right away.

If you like the idea, give us a star to show your support for the project ⭐

## What can this help with?

There are multiple modules we actually provide to boost the performances of your AI systems:

- [x]  [Speedster](https://github.com/nebuly-ai/nebuly/tree/main/optimization/speedster): Automatically apply the best set of SOTA optimization techniques to achieve the maximum inference speed-up on your hardware.

- [x]  [Nos](https://github.com/nebuly-ai/nos): Automatically maximize the utilization of GPU resources in a Kubernetes cluster through real-time dynamic partitioning and elastic quotas - Effortless optimization at its finest!

- [x]  [ChatLLaMA](https://github.com/nebuly-ai/nebuly/tree/main/optimization/chatllama): Build faster and cheaper ChatGPT-like training process based on LLaMA architectures.

- [x]  [OpenAlphaTensor](https://github.com/nebuly-ai/nebuly/tree/main/optimization/open_alpha_tensor): Increase the computational performances of an AI model with custom-generated matrix multiplication algorithm fine-tuned for your specific hardware.

- [x]  [Forward-Forward](https://github.com/nebuly-ai/nebuly/tree/main/optimization/forward_forward): The Forward Forward algorithm is a method for training deep neural networks that replaces the backpropagation forward and backward passes with two forward passes.

## Next modules and roadmap
We are actively working on incorporating the following modules, as requested by members of our community, in upcoming releases:

- [ ]  [Promptify](https://github.com/nebuly-ai/nebullvm/blob/main/apps/extract/promptify): Effortlessly personalize large APIs generative models from OpenAI, Cohere, HF to your specific context and requirements.
- [ ]  [CloudSurfer](https://github.com/nebuly-ai/nebuly/tree/main/optimization/cloud_surfer): Automatically discover the optimal cloud configuration and hardware on AWS, GCP and Azure to run your AI models.
- [ ]  [OptiMate](https://github.com/nebuly-ai/nebuly/tree/main/optimization/optimate): Interactive tool guiding savvy users in achieving the best inference performance out of a given model / hardware setup.
- [ ]  [TrainingSim](https://github.com/nebuly-ai/nebullvm/blob/main/apps/simulate/training_sim): Easily simulate the training of large AI models on a distributed infrastructure to predict training behaviours without actual implementation.

## Contributing
As an open source project in a rapidly evolving field, we welcome contributions of all kinds, including new features, improved infrastructure, and better documentation. If you're interested in contributing, please see the [linked](https://docs.nebuly.com/contributions/) page for more information on how to get involved.

---

<p align="center">
  <a href="https://discord.gg/77d5kGSa8e">Join the community</a> |
  <a href="https://www.nebuly.com/">Visit the website</a>
</p>
